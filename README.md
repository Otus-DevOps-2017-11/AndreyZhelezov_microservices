#######################################################

# monitoring-2. Домашнее задание #23.

Для ДЗ подготовлено окружение в виде docker-host: **vm1** на котором развернуты контейнеры микросервисов приложения **reddit**, а так же мониторинг состоящий из контейнеров Prometheus и node-exporter контейнер для наблюдения за метриками **docker-host**.
## Мониторинг контейнеров.
Инфраструктура была преобразована. Описания контейнеров приложеня остались в файле **docker-compose.yml**, а описания контейнеров для реализации мониторинга (prometheus и node-exporter) были вынесены в файл **docker-compose-monitoring.yml**.
В файл **docker-compose-monitoring.yml** добавлен сервис _cadvisor_, job с новым сервисом добавлен в конфигурацию Prometheus, образ контейнера с новой конфигурацией **prometheus.yml** пересобран. После чего инфраструктура контейнеров приложения и мониторинга были пересозданы. Так же в настройки фаервола GCP добавлено правило для доступа к веб-интерфейсу cadvisor'а. Далее попав в UI cadvisor'а убеждаемся в наличии информации о контейнерах запущеных на нашем docker-host и видим информацию по каждому контейнеру. Так же убеждаемся в наличии по пути _IP:8080/metrics_ метрик для Prometheus, и что эти метрики доступны в интерфейсе самого Prometheus.
## Визуализация метрик.
В файл **docker-compose-monitoring.yml** добавлен сервис _grafana_. Инфраструктура с новым контейнером пересобрана. Добавлено правило фаервола GCP для доступа к веб-интерфейсу Grafana, и переходим в UI.
В UI Grafana производим следующие действия:
* Настраиваем источник данных Prometheus.
* Скачиваем из библиотеки комьюнити и схраняем себе файл дашборда **DockerMonitoring.json** для мониторинга Docker. Файл сохраняем в новую директорию **monitoring/grafana/dashboards** репозитория.
* Импортируем скачанный дашборд в Grafana, используя ранее настроенный источник данных Prometheus.
В результате в UI Grafana мы видим информацию о состоянии хоста и контейнеров.
### Метрики приложения.
В конфигурацию Prometheus добавлена _job_name: 'post'_ которая будет собирать метрики с сервиса post нашего приложения. Образ контейнера и инфраструктура мониторинга пересобраны. 
Далее создан дашборд _UI_Service_Monitoring_ в который добавлен график _UI HTTP Requests_ счетчика запросов в разрезе URI-путей на которые они направлены. Для проверки работы данного графика создано несколько постов и коментов в приложении. 
Добавлен график _Rate UI HTTP Requests with Error_ отображения скорости изменения счетчика ошибочных запросов. Этот график наглядно отображает наличие ошибочных запросов в определенный период, в разрезе путей на которые они направлены. График постоен из мерики счетчика ошибочных запросов в качестве аргумента функции rate(). Дашборд сохранен в текущей версии.
Функция rate() использована для преобразования первого графика в более информативный график изменения счетчика количества запросов, аналогично графику _Rate UI HTTP Requests with Error_. Обновленный дашборд сохранен в новой версии.
Изучены тип метрики _histogram_ и понятие _перцентиль_.
Добавлен график _HTTP response time 95th percentile_, отображающий время ответа на запросы у большинства запросов в период времени. Дашборд сохранен и экспортирован в файл **UI_Service_Monitoring.json**.
### Метрики бизнес логики.
Для мониторинга бизнес логики создан дашборд _Business_Logic_Monitoring_ в который добавлен график отображения скорости изменения счетчика постов в приложении. Далее добавлен аналогичный график для коментов. Данны графики наглядно отбражают активность пользователей в промежуток времени. Дашборд сохранен и экспортирован в файл **Business_Logic_Monitoring.json**.
## Алертинг.
Создан образ контейнера alertmanager из файла **monitoring/alertmanager/Dockerfile**. Образ запушен на докер-хаб (ссылки внизу). В качестве мэсенджера использован именной канал в Слаке, для чего на нем настроен web-hook, его API URL использован для настройки alertmanager. Сам alertmanager запущен в ещё одном контейнере, для чего был собран образ с конфигом в котором настроен web-hook и добавлен новый сервис в **docker-compose-monitoring.yml**. В Prometheus добавлена конфигурация алерта, путем добавления файла **monitoring/prometheus/alerts.yml** в образ контейнера. После пересоздания инфраструктуры мониторинга в интерфейсе Alerts появился наш настроенный алерт. Так же его можно посмотреть в собственном UI Alertmanager'а, для чего добавлено правило фаервола GCP для доступа к веб-интерфейсу Alertmanager'а. Для проверки работы алерта был выключен контейнер post, и через минуту получено оповещение об этом в Слак чат. 
## Результаты работы.
Созданные в результате выполнения ДЗ образы находятся по следующим ссылкам:
https://hub.docker.com/r/azhelezov/comment/
https://hub.docker.com/r/azhelezov/post/
https://hub.docker.com/r/azhelezov/ui/
https://hub.docker.com/r/azhelezov/prometheus/
https://hub.docker.com/r/azhelezov/alertmanager/

#######################################################

# monitoring-1. Домашнее задание #21.

## Подготовка инфраструктуры.
Для выполнения ДЗ в GCP создан docker-host **vm1** и окружение docker-client переключено на работу с ним. С помощью готового образа с Docker Hub был запущен контейнер с Prometheus.
Подключившись к веб-интерфейсу Prometheus, изучаем интерфейс. И останавливаем контейнер.
### Настройка Prometheus и приложения.
Структура каталогов в репозитории приведена к новому виду, описанному в ДЗ-презентации.
В новом каталоге ./monitoring/Prometheus/ создан Dockerfile для билда образа контейнера с сервером **Prometheus**, настроенным на мониторинг микросервисов нашего приложения. Так же был создан фай конфигурации Prometheus'а **monitoring/prometheus/prometheus.yml**. Далее из указанного контекста был собран образ. 
Так же собраны образы микросервисов с healthcheck'ами.
Файл docker/docker-compose.yml был доработан. В него был добавлен сервис для запуска контейнера с Prometheus, удалены директивы build для сборки имеджей микросервисов, добавлен volume _prometheus_data_, добавлена секция networks в определение сервиса Prometheus. Так же в файле .env изменены ссылки на версии микросервисов на "latest". После запуска контейнеров проверено что приложение работает, и нитерфейс Prometheus отдает 3 endpoint'а.
## Работа с Prometheus.
### Мониторинг сервисов.
Для тестирования встроенных в код микросервисов healthckeck'ов в интерфейсе Prometheus'а были открыты графики соответствующих метрик и проведены поочередные отключения и включения контейнеров с микросервисами. В процессе были отмечены соответствующие переключения метрик в состояние "0", т.е. ошибки доступности как самих сервисов так и зависимых от них.
### Сбор метрик хоста.
Для сбора метрик хоста создан ещё один контейнер с приложением экспортером **node-exporter**. Описание этого контейнера было добавлено в файл docker/docker-compose.yml. В описание данного контейнера добавлены сети сервисов.
В конфиг Prometheus добавлен ещё один job, в котором указываем ссылку на интефейс с метриками. Так же потребовалась пересборка образа с Prometheus, и перезапуск нашей инфраструктуры с новым образом Prometheus. 
После перезапуска инфраструктуры в интерфейсе Prometheus появился новый endpoint **node**. Наблюдая за метрикой _node_load1_ проведено некоторое колличество нагрузочных тестов. В результате были  отмечены соответствующие изменения графика нагрузки на CPU docker-хоста **vm1**.
## Результаты работы.
Созданные в результате выполнения ДЗ образы находятся по следующим ссылкам:
* https://hub.docker.com/r/azhelezov/ui/
* https://hub.docker.com/r/azhelezov/post/
* https://hub.docker.com/r/azhelezov/comment/
* https://hub.docker.com/r/azhelezov/prometheus/


#######################################################

# docker-7. Домашнее задание #20.

## Расширение пайплайна.
Для дальнейшей работы в текущем ДЗ№20 был создан новый проект в группе homework Gitlab CI: **example2**. По причине смены динамического внешнего IP-адреса хоста с Gitlab, потребвалось пересоздавать контейнер _gitlab-ce_ и справленным адресом в _docker-compose.yml_. Так же потребовалось удалить старый _git remote_ и создать новый для проекта из ДЗ№19. Так же новый _git remote_ добавлен для нового проекта **example2**, и к этому же проекту был подключен ранее созданный runner.
### Создание окружений.
Стадия _deploy_job_ в описании пайплайна изменена таким образом что на данной стадии создается откружение _dev_ и на него происходит деплой любых изменений в коде. После пуша изменений на remote **gitlab**, пайплайн отработал без ошибок и появилось окружение dev в списке окружений проекта.
По аналогии с _dev_ окружением, в пайплайн были добавлены определения окружений _staging_ и _production_, которые деплоятся с условием нажатия на кнопку запуска. Тестирование прошло успешно.
### Условия и ограничения.
В окружения _staging_ и _production_ добавлены условия запуска этих job'ов только в случае наличия тэга c версией в semver-формате. Был создан комит без указания тегов и пуш этого комита вызвал пайплайн без job'ов _staging_ и _production_. После чего текущий коммит был изменен, снабжен таким тегом ( 2.4.10 ) и после этого изменения вместе с тегами запушены в remote проекта на gitlab. На этот раз проект собрался со всеми job'ами.
### Динамические окружения.
Для тестирования функционала создания динамических окружений в пайплайн добавлено определение оклужения с динамически назначаемыми именами. Частью составного имени окружения является название ветки полученное из переменной окружения gitlab. При этом условием создания окружения является наличе новой ветки не с названием _master_. Для проверки была создана ветка **new-feature** она была отправлена на удаленную ветку **new-feature** в репозитории gitlab что вызвало запуск пайплайна с созданием окружения _branch/new-feature_.

#######################################################

# docker-6. Домашнее задание #19.

## Подготовка Gitlab CI.
На ресурсах GCP была создана виртуальная машина **docker-gitlab-host** с помощью docker-machine с использованием драйвера google. На ВМ было дополнительно установлено docker-compose, а так же созданы директории необходимые для работы gitlab и установлен gitlab-ce в контейнере (omnibus). После чего интеррфейс gitlab стал доступен по внешнему адресу виртуальной машины из интернет.
После смены пароля root, успешно авторизовавшись, отключили регистрацию новых пользователей.
## Создание проекта.
Через интерфейс Gitlab была создана группа **homework** и проект **example**. После чего проект был подключен как удаленный ресурс к git-репозиторию в ветке docker-6.
Создан файл описания пайплайна _.gitlab-ci.yml_ закомичен в репозиторий и запушен в Gitlab, в результате чего в интерфейсе Gitlab появился наш пайплайн в статусе **pending/stuck**. 
### Запуск и регистрация Runer'а.
В веб интерфейсе Gitlab CI в настройках CI/CD находим и копируем токен ранера, с помощью которого привяжем созданный позже ранер и наш проект.
Далее создаем ранер на **docker-gitlab-host** из докер-образа. При запуске ранера регистрируем его в проекте, используя токен скопированый из интерфейса проекта ранее, а так же http-url по которому доступен наш Gitlab CI. Проверено что новый ранер появился в настройках проекта.
Пробный запуск пайплайна прошел успешно.
## Работа с проектом в Gitlab.
Добавляем тестирование приложения в наш пайплайн.
Исходный код приложения _reddit_ добавлен в локальный репозиторий, и запушен в удаленный gitlab репозиторий **docker-6**. 
Описание пайплайна в _.gitlab-ci.yml_ изменено:
* добавлен docker-image с ruby который будем использовать в тестировании;
* добавлена переменная окружения, указывающая на расположение БД;
* добавлены предварительные скрипты, для установки зависимостей;
* добавлены docker-сервис mongo и вызов скрипта тестирования _simpletest.rb_ в test_unit_job.

В рабочий каталог приложения **reddit** добавлен скрипт _simpletest.rb_.
В **reddit/Gemfile** добалена подключаемая библиотека для ruby: _rack-test_.
После пуша в удаленный репозиторий gitlab docker-6, пайплайн успешно запустился и протестировал наше приложение.

#######################################################

# docker-4. Домашнее задание #17.
## Работа с сетью Docker.
### Сетевой драйвер "none".
Для проверки работы сетевого драйвера "none" использован образ с предустановленными сетевыми утилитами **joffotron/docker-net-tools**. На его основе создан и запущен контейнер. В процессе его работы, подключившись к контейнеру в интерактивном режиме видим что внутри контейнера есть loopback интефейс.
### Сетевой драйвер "host". 
Для проверки работы сетевого драйвера "host" контейнер запущен с параметром _--network host_. После чего были сравнены выводы двух команд _ifconfig_ для запущеных с созданного контейнера "net_test" и затем с хоста "docker-host". Т.к. драйвер "host" использует сетевой стек хоста внутри контейнеров, выводы этих двух команд почти идентичны, вывод _ifconfig_ изнутри контейнера только дополнительно указывает к какому порту прибинден текущий сетевой стек контейнера:
```
...
docker0   Link encap:Ethernet  HWaddr 02:42:DB:90:4C:57       |	docker0   Link encap:Ethernet  HWaddr 02:42:db:90:4c:57  
          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:25	          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:25
          inet6 addr: fe80::42:dbff:fe90:4c57%*32691*/64 Scope: |	          inet6 addr: fe80::42:dbff:fe90:4c57/64 Scope:Link
...
```  
Так же для проверки работы драйвера "host" были несколько раз созданы контейнеры nginx: _docker run --network host -d nginx_. В результате каждый следующий контейнер создаваясь пытался занять всё тот же порт tcp/80 хоста что и предыдущий, в результате процесс nginx контейнера завершался с ошибкой, а с ним и сам контейнер переставал работать.
### Сетевой драйвер "bridge". 
#### Работа внутри одной сети.
Для проверки работы серевого драйвера "bridge" были запущены контейнеры с микросервисами нашего приложения, как и в прошлом ДЗ, но без указания сетевых алиасов, поэтому сервисы не могли общаться друг с другом. После указания алиасов при перезапуске контейнеров приложение заработало.
#### Работа в двух сетях.
Были созданы две сети типа "bridge": _back_net_ для контейнера MongoDB  и _front_net_ для ui. Таким образом у фронтэнда не будет доступа к БД. Промежуточные сервисы приложения (их контейнеры) были добавлены изначально только в сеть _back_net_, но при таком варианте у сервисов post и comment небыло доступа к БД. Поэтому уже после создания и запуска контейнеров они (контейнеры post и comment сервисов) были добавлены и во вторую сеть _front_net_. После чего приложение успешно заработало.
## Docker-compose.
На рабочем компьютере установлена утилита **docker-compose**. В каталоге reddit-microservices репозиория с ДЗ создан файл docker-compose.yml. В него добавлено описание всей инфраструктуры контейнеров приложения в т.ч. volume, сеть и сами контейнеры. Далее с использованием этого файла была создана инфраструктура, предварительно была определена переменная окружения $USERNAME с именем пользователя в docker-hub. Вход на веб-интерфейс приложения показал что оно работает. Однако при попытке перейти в созданный пост получаем ошибку приложения. И это не удивительно в docker-compose.yml, у сервиса post_db как минимум не хватает сетевого алиаса comment_db, поэтому сервис comment не знает как обратиться к базе данных. Исправил это недоразумение тем что добавил алиасы в раздел _networks_ сервиса _post_db_. 
### Самостоятельное задание.
1. Описание проекта в файле docker-compose.yml изменено т.о. что сервисы разделены по двум сетям  _back_net_ и _front_net_ и сервису mongo_db назначены необходимые сетевые алиасы. Тестирование показало что приложение полностью работоспособно.
2. Параметризированы некоторые значения в файле docker-compose.yml:
* USERNAME - имя пользователя,
* H_PORT_UI - порт хоста на который сбинден порт сервиса UI,
* C_PORT_UI - порт сервиса UI внутри контейнера,
* UI_VER - версия образа для сборки контейнера ui,
* POST_VER - версия образа для сборки контейнера post,
* COMMENT_VER - версия образа для сборки контейнера comment.
3. Переменные окружения применяемые в docker-compose.yml описаны в файле .env проекта. Образец такого файла выложен в репозиторий под именем .env.example
4. Переменные применяются из этого файла автоматически при условии запуска docker-compose из директории в которой он лежит.

#######################################################

# docker-3. Домашнее задание #16.
Все рабочие файлы текущего репозитория перенесены в каталог **./docker-monolith/**. В корень репозитория добавлен файл **.dockerignore** с исключениями из контекста в т.ч. указанной директории.
## Разбиение приложения на микросервисы.
В корень репозитория распакован каталог с приложением: **reddit-microservices**.
В каталоги с именами частей приложения: **post**, **comment**, **ui** помещены Dockerfile-ы с описанием образов. На основе этих Docerfile были контейнеры для запуска приложения, каждый сервис в своём контейнере. Так же был скачан контейнер с последней версией MongoDB. P.S. сборка контейнера ui началась с первого шага.
Далее была создана одноранговая сеть _reddit_ средствами docker. В неё будут добавлены наши контейнеры с приложением. 
Все контейнеры были запущены в работу в детач режиме. Каждому контейнеру кроме ui присвоены network-алиасы. Они исполняют роль доменных имен, по которым к контейнерам можно обращаться.
Для проверки я подключился к внешнему адресу docker-host на экспонированый порт 9292 к веб-интерфейсу приложения и создал один пост.
## Оптимизация контейнеров.
В процессе пересборки образов неоднократно возникала ошибка нехватки памяти на docker-host. Поэтому тип машины docker-host на GCP был изменен на g1-small.
Dockerfile сервиса ui был изменен и на его основе был собран образ на >300Mb меньше образа первой версии:
```
azhelezov/ui            2.0                 75b9f41ab998        32 seconds ago      453MB
azhelezov/ui            1.0                 630b8791ede8        2 days ago          775MB
```
Данным образом был создан новый контейнер ui и приложение запущено с этим новым контейнером. После перезапуска контейнера с MongoDB все данные записанные ранее в БД пропали, и поста созданого на предыдущем шаге небыло. Чтобы это исправить был создан иподключен volume для хранения данных. Чтобы смонтировать данный volume в директорию БД в контейнере mongo, все контейнеры были перезапущены, а в команду запуска контейнера mongo была добавлена привязка созданного volume к этому контейнеру опция _-v reddit_db:/data/db_.
Для проверки работы volume на запущеном приложении был создан пост в веб-интерфейсе и далее контейнеры были снова перезапущены. На этот раз после перезапуска информация в БД сохранилась - пост был на месте.

#######################################################

# docker-2. Домашнее задание #15.
## Исходные данные.
ОС Ubuntu Linux (xenial) 16.04.
Установлена docker-machine версии 0.13.0.
В GCP создан новый проект: docker-XXXXXXXX. К нему перенастроена конфигурация gcloud.
## Работа с docker-host.
### Создание docker-host.
На GCP с помощью gcloud создана и запущена ВМ с названием docker-host. Проверка командой _docker-machine ls_ показала что хост создался и что он запущен.
Изменено окружение docker-machine на демона docker-host. Таким образом все команды docker будут выполняться демоном на docker-host.
### вопрос со \*.
Разница в выводе двух комманд:
* docker run --rm -ti tehbilly/htop;
* docker run --rm --pid host -ti tehbilly/htop;

,состоит в том что в первом (дефолтном) варинате мы увидим процесс htop созданного контейнера и только его, а во втором случае мы увидим все процессы хоста с которого запускаем контейнер. Происходит так потому что по дефолту контейнер запускается со своим **PID Namespace** и единственный процесс в нем получает PID=1, а во втором случае из-за опции _--pid=host_ процессу из контейнера выделяется PID из общего namespace хоста. И вцелом из контейнера мы получаем доступ к **PID namespace** хоста и видим все его процессы.
### Создание образа с приложением.
В репозитории создан файл конфигурации MongoDB: mongod.conf.
Создан bash-скрипт запуска приложения: start.sh.
Создан файл переменных в котором задается переменная IP-адреса хоста с БД: db_conf. 
Создан файл Dockerfile, в который будет размещено опсание нашего образа. В Dockerfile добавлены следующие комманды:
* задание исходного образа
```
FROM ubuntu:16.04
```
* обновление кэша репозиториев и установка пакетов для работы приложения
```
RUN apt-get update
RUN apt-get install -y mongodb-server ruby-full ruby-dev build-essential git
RUN gem install bundler
```
* закачка приложения в контейнер
```
RUN git clone https://github.com/Artemmkin/reddit.git
```
* копирование файлов конфигурации в контейнер
```
COPY mongod.conf /etc/mongod.conf
COPY db_config /reddit/db_config
COPY start.sh /start.sh
```
* установка зависимостей и насройка прав доступа к скрипту запуска приложения
```
RUN cd /reddit && bundle install
RUN chmod 0777 /start.sh
```
* выполнение старта приложения при старте контейнера
```
CMD ["/start.sh"]
```
На docker-host cоздан образ контейнера **reddit:latest** из описания в Dockerfile. На основе образа **reddit:latest** создан и запущен контейнер. 
При попытке подключиться к приложению, запущеному в контейнере, по внешнему адресу хоста на порт 9292 соединение не устанавливалось из-за отсутствия соответствующего правила фаервола в VPC-разделе GCP. После добавления правила доступ к приложению по http/9292 появился.
## Работа с Docker Hub.
Создана учетная запись на Docker Hub. С использованием этой учетной записи осуществлен логин из контекста docker к Docker Hub.
Далее созданный ранее образ **reddit:latest** был залит на Docker Hub для дальнейшего использования. Для этого локальному образу образу был присвоен тэг удаленного образа **<login_name>/otus-reddit:1.0** и сделан push к удаленному образу.

#######################################################

# docker-1. Домашнее задание #14.
## Исходные данные
ОС Ubuntu Linux (xenial) 16.04.
Установлена community версия Docker 17.12. Тестовый запуск показал что клиентская и серверная часть работают правильно.
## Операции с контейнерами.
После запуска тестового контейнера "Hello world" испробованы команды Docker для вывода списка контейнеров, образов. Затем на основе образа Ubuntu:16.04 было создано два контейнера и опробованы различные опции команды *docker run*.
Далее были опробованы команды для подключения к уже запущеному контейнеру и комбинации выхода из контейнера без его закрытия.
После чего был создан образ на основе контейнера, модифицированного созданием файла /tmp/file. Вывод команды docker images в котором виден созданный образ сохранен в файл docker-1.log.
### задание со *
Сравнение вывода двух команд docker inspect <image_ID> и docker inspect <container_ID> дописано в файл docker-1.log.

Далее испробованы команды завершения работы контейнеров, отображения занимаемого дискового пространства, удаления контейнеров и образов с опциями.

# AndreyZhelezov_microservices
