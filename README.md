#######################################################

# kubernetes-5. Домашнее задание #31.

## Подготовка.
Подготовлен кластер GKE из двух нод *n1-standard-1** в пуле **default**, и одной ноды **n1-standard-2** в пуле **bigpool**. Произведены настройки кластера.
Установлен ingress-контроллер nginx. Его внешний адрес для разименования нужных имен добавлен в /etc/hosts.

## Настройка мониторинга.
Из helm-чарта установлен Prometheus. По адресу **http://reddit-prometheus/** доступен интерфейс Prometheus.
Омечено наличие endpoint'ов метрик API-сервера, метрики нод с cadvisor’ов и сам prometheus. Изучены настройки использующие **service discovery** для настройки endpoint'ов, конфиг шифрованного подключения к Kubernetes API и механизм подмены лэйблов k8s на лэйблы понятные Prometheus.
Для сбора информации о сущностях k8s включен сервис **kube-state-metrics**, и после апгрейда чарта **prom** сетрики стали собираться. Аналогичным образом влючены node-exporter'ы.
### Запуск и мониторинг приложения.
Из helm-charta было запущено reddit-приложение в трех окружениях: default, production и staging.
Изменениями в конфиге **custom_values.yml** чарта **prom** настроен поиск endpoint'ов приложения с использованием **service discovery**. Преобразованием лейблов k8s в лейблы Prometheus настроена идентификация endpoint'ов в интерфейсе Prometheus.
#### Самостоятельное задание.
Таргет **reddit-endpoints** заменен на три таргета по компонентам приложения: **post-endpoints**, **comment-endpoints**, **ui-endpoints**.
### Визуализация.
Из helm-chart'а развернута Grafana. Prometheus подключен в качестве источника данных. 
Добавлен дашборд для мониторинга k8s.
Добавлены дашборды приложений созданные в ДЗ по мониторингу. С помощью механизма templating'а в дашборды добавлен выбор окружения с которого отображаются графики. Полученные дашборды сохранены в репозиторий.
Добален дашборд для визуализации метрик **cadvisor** и **kube-state-metrics**.

## Настройка логирования.
Для централизованного логирования будет использоваться EFK-стек. Для его разворачивания самой мощной ноде кластера добавлен ярлык _elastichost=true_.
Создан каталог **efk** в который помещены yaml-манифесты для разворачивания компонент EFK-стека:
```
./efk/
├── es-pvc.yaml
├── es-service.yaml
├── es-statefulSet.yaml
├── fluentd-configmap.yaml
└── fluentd-ds.yaml
```

Стек запущен, кнему так же добавлена Kibana которую развернули из готового helm-чарта.
### Задание со *.
Для разворачивания EFK-стека вместе с Kibana был создан чарт:
```
efk/
├── charts
│   └── kibana-0.1.1.tgz
├── Chart.yaml
├── requirements.lock
├── requirements.yaml
├── templates
│   ├── es-pvc.yaml
│   ├── es-service.yaml
│   ├── es-statefulSet.yaml
│   ├── fluentd-configmap.yaml
│   └── fluentd-ds.yaml
└── values.yaml
```

Работа чарта протестирована с предварительным удалением инфраструктуры EFK-стека развернутой с помощью kubectl.

#######################################################

# kubernetes-4. Домашнее задание #31.

## Подготовка.
Установлен и сконфигурирован пакетный менеджер **Helm** для управления релизами на GKE-кластере Kubernetes.
Путем применения манифеста **tiller.yml** была запущена серверная часть пакетного менеджера - **Tiller**. Так же **Tiller** был проинициализирован для **Helm**.
## Настройка структуры Chart'ов.
Для работы с пакетами(Chart) был создан каталог **kubernetes/Charts** в котором размещена структура каталогов пакетного менеджера. Далее все работы выполнялись относительно этого каталога.
Манифесты, относящиеся к запуску и настройке всего что относится к **ui**, были перенесены в каталог **ui/temlates**, и переименованы соответствующим образом для применения их при релизах.
## Релизы.
Был произведен тестовый релиз **test-ui-1** единичного экземпляра микросервиса ui, а после параметризации некоторых значений манифестов тестовый запуск трех разных релизов с разными именами: **ui-1**, **ui-2** и **ui-3**. Благодаря параметризации значений таких параметров как имя сущности в манифесте и её метаданные, напимер релиз, переменными _.Release.Name_ и _.Chart.Name_ и были запущены несколько сущностей, не конфликтуя др. с др. Позже были параметризированы порты сервиса *ui* и его образ.
Были добавлены чарты для **post** и **comment**.  
Чарты параметризированы с использованием функций-helper'ов, которые заменили простое определение переменной _{{ .Release.Name }}-{{ .Chart.Name }}_ на _{{ template "<service_name>.fullname" . }}_.
### Релиз reddit-приложения.
Был создан чарт **reddit** объединяющий все микросервисы в единое приложение. Дополнительно создан и инициирован файл зависимостей, благодаря которому подготовлены чарты микросервисов и mongodb. Тестовый запуск приложения прошел успешно. 
Однако приложение не работало т.к. сервис ui не знает адресов сервисов post и comment. Поэтому в **ui/templates/deployments.yaml** были добавлены, зависящие от имени релиза, переменные окружения для имен хостов _post_ и _comment_.

## Gitlab + Kubernetes.
Стянут с Gitlab каталог чарта для разворачивания Kubernetes **gitlab-omnibus**. Версия чарта была взята 0.1.37 т.к. в репозитории не было упомянутой в задаче 0.1.36.
При настройке и запуске данного чарта возникли проблемы. Конфиги в манифест-файлах не вполне соответствовали gist-файлам в ДЗ. И настроеная конфигурация не заработала - выдавались ошибки 500 при переходе на http://gitlab-gitlab/groups. Поэтому из исходников https://gitlab.com/charts/charts.gitlab.io была собрана версия 0.1.36 и релиз был обновлен. После этого ошибок не возникает. 
### Заведение проектов и самостоятельное задание.
После запуска Gitlab, в него были добавлены 4 проекта: reddit-deploy, ui, post, comment. Для всех проектов созданы локальные репозитории и сбиндены с ориджин репозиториями в нашем Gitlab'е, после чего исходные файлы ui, post и comment были добавлены в локальные репозитории, а в локальный репозиторий reddit-deploy был скопирован чарт приложения. Все локальные репозитории были запушены в ориджин репозитории проектов.
Далее в локальные репозитории проектов ui, post и comment были добавлены конфиги пайплайнов для проектов Gitlab CI/CD: **.gitlab-ci.yml**. После пуша изменений в Gitlab проекты успешно отработали данные пайплайны.
### Создание временного окружения и самостоятельное задание.
С целью предоставить возможность разворачивания временного окружения в k8s при комите в ветку **feature/3**, в проекте каждой из трех частей приложения были внесены изменения в пайплайн. Добавлены функции создания и удаления временного окружения, а так же сопутствующие функции вроде установки helm и tiller для целей развертывания окружений. 
Тестирование прошло успешно. Окружения создавались и удалялись по кнопке.
### Создание окружений staging и production проекта reddit-deploy.
Создан конфиг **.gitlab-ci.yml** для проекта _reddit-deploy_, измененный т.о. репозиторий запушен в проект. В результате запустился пайплайн создающий окружение **staging** и по кнопке **production**.
Тестирование показало что окружения создаются и приложения становятся доступны.

#######################################################

# kubernetes-3. Домашнее задание #30.
## Настройка Load Balanser.
Для доступа к сервису **ui** был настроен тип доступа _LoadBalanser_, который заменил собой тип доступа _NodePort_. В результате применения измененного сервиса запустился дополнительный POD с сервисом _LoadBalanser_ внутри кластера и открыл на внешнем интернет-адресе порт tcp:80 направляя таким образом трафик на сервис ui. 
## Настройка Ingress Controller и Ingress.
Для настройки Ingress была использованан возможность, предоставляемая GKE, которая называется **Балансировка нагрузки HTTP**. Для её применения был создан манифест **ui-ingress.yml** который был настроен на простую балансировку нагрузки на сервисы ui с внешнего порта tcp:80. Было отмечено что Ingress направляет трафик с внешнего порта на сервис череж сконфигурированный в сервисе _NodePort_.
После успешной настройки Ingress'а из конфигурации сервиса ui был убран ненужный уже далансировщик LoadBalanser.
Далее в правила **ui-ingress.yml** было внесено промежуточное звено http/L7-балансировки. Впрочем фактически оно ничего не изменило в части доступа к приложению, правило охватывает все URI и направляет трафик на наш сервис ui.
## Настройка шифрации HTTP. Secret и TLS.
С помощью утилиты **openssl** была создана пара tls-ключ и tls-сертификат, эту пару будем использовать для организации https подключения к нашему приложению.
С помощью команды **kubectl create secret...** был создан секрет **ui-ingress** типа _tls_ и в качестве параметра был передан созданый нами на предидущем шаге самоподписанный сертификат. В самом ingress'е запрещён http трафик и подключен созданный нами секрет, путем правки и применения файла ui-ingress.yml. Через некоторое время удалось успешно зайти на интерфейс приложения по протоколу https через ingress.
### Задание со *.
Для настройки секрета к ingress'у _ui-ingress_ был создан yaml-файл манифеста ingress-secret.yml. 
Для проверки работы секрета заданного манифестом, текущий секрет был удален, удален и пересоздан заново сам ingress. После чего я убедился в отсутствии секрета в окружении **dev** как такового, и доступа к приложению по Интернет. Затем секрет был создан с прменением манифеста _kubectl apply -n dev -f ingress-secret.yml_, а так же был создан заново ingress. Через некоторое время доступ по https на страницу по адресу ingress'а заработал.
## Настройка Network Policy. 
В рамках задачи разграничения доступа сервисов кластера друг к другу были произведены следующие шаги.
Был задействован сетевой плагин **Calico**, вместо **Kubelet**. С его помощью можно настроить и применять политики сетевого доступа к сервисам.
Был создан файл-манифест **mongo-network-policy.yml** в котором доступ снаружи к подам сервиса mongo был вцелом ограничен, но разрешен для сервиса _comment_, а позднее и _post_, в качестве самостоятельного подзадания. До добавления правила доступа для post-сервисов приложение не работало в части сервиса post.
## Настройка хранения данных. 
Было проверено что данные приложения (посты и коменты) исчезают после удаления и возврата деплоя mongo. Так и должно быть втекущей настройке системы хранения *emptyDir*.
Для решения проблемы был создан удаленный диск для хранения данных. Диск создан на ресурсах GCP: gcePersistentDisk с именем **reddit-mongo-disk**.
В **mongo-deployment.yml** внесены изменения, а именно созданный **reddit-mongo-disk** был подключен вместо хранилища типа *emptyDir*. Проверка удалением и созданием заново деплоя mongo показала что данные никуда не исчезли, т.к. **reddit-mongo-disk** был просто переподключен.
### PersistentVolumes.
В качестве развития системы хранения был создан PersistentVolume, доступный для всего кластера. Для этого было создано и применено его описание **mongo-volume.yml** в котором указано использовать созданный ранее **reddit-mongo-disk**. Пока данный PersistentVolume(PV) в кластере не используем.
### PersistentVolumeClaims
Для того чтобы задействовать созданный PV сконфигурирован запрос на его использование PersistentVolumeClaim(PVC) в манифесте **mongo-claim.yml**. А созданный PVC подключен к деплою mongo. После передеплоя mongo былопроверено что данные никуда не исчезают, и приложение вцелом функционирует.
### Динамическое создание хранилищ.
Для выделения хранилища по запросу PVC был настроен компонент GKE **StorageClass** под названием **fast**. Для этого был создан манифест **storage-fast.yml** в котором указан провайдер и тип хранилища, затем был создан манифест PVC **mongo-claim-dynamic.yml** в котором в качестве хранилища указан наш **StorageClass** **fast**, а сам PVC назван **mongo-pvc-dynamic**. Ну и наконец в деплое mongo PVC подключаемый в качестве volume'а был заменен с **mongo-pvc** на **mongo-pvc-dynamic**, и деплой пересздан. Работа приложения проверена.
Созданные в кластере PV:
```
$ kubectl get pv -n dev
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                   STORAGECLASS   REASON    AGE
pvc-00ea339c-372a-11e8-9a7e-42010a8400d5   15Gi       RWO            Delete           Bound       dev/mongo-pvc           standard                 3h
pvc-9b2be53e-372f-11e8-9a7e-42010a8400d5   10Gi       RWO            Delete           Bound       dev/mongo-pvc-dynamic   fast                     2h
reddit-mongo-disk                          25Gi       RWO            Retain           Available                                                    4h
```

#######################################################

# kubernetes-2. Домашнее задание #29.

## Подготовка.
На рабочем компьютере были установлены **kubectl** и **minikube**. Был развенут minikube-кластер kubernetes. Текущий конфиг(контекст) **kubectl** был переключен на minikube-кластер.
## Minikube-кластер.
### Разворачивание приложения.
В кластере было развернуто приложение Reddit. Для этого были созданы yaml-манифесты для конфигурирования всех необходимых сущностей kubernetes.
Рабочая нагрузка в виде подов с сервисами нашего приложения сконфигурирована манифестами **\*-deployment.yml**.
Сетевое обнаружение севрисов сконфигурировано манифестами **\*-service.yml**. В т.ч. созданы алиасы для сетевого обнаружения сервиса mongo севрисами post и comment: **post-mongodb-service.yml** и **comment-mongodb-service.yml**.
В сервис **ui-service.yml** была добавлена жиректива **NodePort** которая обеспечивает внешний доступ к порту приложения внутри кластера указанному в конфиге сервиса как **targetPort: 9292**. Т.о. был опубликован порт интерфейса приложения для доступа к нему извне.
Все файлы находятся в директории **./kubernetes/** проекта.
### Аддон Dashboard и namespace'ы.
На minikube-кластере был развернут аддон **Dashboard** который позволяет мониторить состояние кластера. Были дополнительно поверхностно изучены и другие доступные аддоны.
Было добавлено новое окружение в minikube-кластер: **dev**. Сделано это при помощи применения манифеста **dev-namespaces.yml**, после прменения которого в minikube-кластере в новом окружении было развернуто наше приложение ещё раз.
## GKE-кластер.
На платформе Google Kubernetes Engine (GKE) был развернут кластер kubernetes. 
Для работы с кластером был сконфигурирован kubectl, в него заведен новый конфиг (кластер, пользователь и контекст) для созданного только что GKE-кластера.
Приложение Reddit аналогичным с minikube образом было развернуто на GKE-кластере, сразу в dev-окружении.
В GCP было создано правило брэндмауэра для доступа к приложению из интернет, а именно был открыт диапазон портов tcp:30000-32767. Для проверки доступа к приложению из интернет, был выяснен номер порта (NodePort) на который был выставлен порт приложения сервиса ui (tcp:9292), и на этом порту на внешнем адресе одной из нод кластера открыт web-интерфейс ui. Скриншот приложен к PR.
### Настройка Dashboard-аддона.
В GKE-кластере так же как и в minicube был забущщен dashboard. Он был включен как компонент кластера под названием "Панель управления Kubernetes". И, хтя проблем с наличием учетной записи **kube-system:kubernetes-dashboard** в кластере не возникло, доступа у этой учетки к чтению состояния кластера не оказалось. Для того чтобы исправить эту ситуацию, была сделана привязка данной учетной записи к роли **cluster-admin** нашего кластера. После чего авторизация в RBAC прошла успешно и dashboard получил доступ к данным о состоянии кластера.

#######################################################

# kubernetes-1. Домашнее задание #28.

## Подготовка.
Создан каталог kubernetes в репозитории. В него размещены файлы-заготовки для описания сущностей **Deployments** для приложений ui, post, comment и mongo.
## Прохождение туториала Kubernetes The Hard Way.
Пройден туториал **Kubernetes The Hard Way**. В результате чего последовательно вручную были созданы основные компоненты кластера Kubrnetes на трех управляющих нодах типа **master** и трех нодах типа **worker**. Ноды развернуты на платформе GCP, к ним настроен балансировщик с постоянным внешним адресом, натсроена утилита kubectl для удаленного управления кластером.
## Самостоятельное задание.
В репозитории ДЗ был создан каталог **kubernetes/kubernetes_the_hard_way** в который были помещены все файлы созданные в процессе прохождения туториала.
Так же на созданном кластере были запущены поды deployment-ов для приложений ui, post, comment и mongo, созданные в первом шаге ДЗ. Они успешно запустились в созданном кластере:

```
$ kubectl get pod -a
NAME                                  READY     STATUS    RESTARTS   AGE
busybox-855686df5d-rll4w              1/1       Running   1          1h
comment-deployment-85f97f7c69-mpd27   1/1       Running   0          22m
mongo-deployment-74cccfb8-v8nxj       1/1       Running   0          22m
nginx-8586cf59-djq8p                  1/1       Running   0          1h
post-deployment-5bfc5d9945-p65qz      1/1       Running   0          22m
ui-deployment-d8f8889-vjs8l           1/1       Running   0          23m
```


#######################################################

# swarm-1. Домашнее задание #27.

## Подготовка кластера Docker Swarm.
На платформе GCP создана ВМ(нода) **master-1**. На ней инициирован кластер docker swarm. Соданы ещё две ноды **worker-1** и **worker-2**. При помощи токена полученого на **master-1** ноды **worker-1** и **worker-2** добалены в кластер с ролью _worker_. 
Для запуска стека был подготовлен и переработан файл конфигурации **docker-compose.yml**. Так же были внесены некоторые корректировки в файл переменных окружения. Запуск стека производился командой *docker stack deploy* и в качестве *--compose-file* указан вывод опции *config* команды *docker-compose*. Такой прием используется потому что *docker stack deploy* не раскрывает переменные из файлов конфигурации docker-compose. В результате был развертнут стек с нашим приложением, состоящим из микросервисов.
## Размещение сервисов.
Для размещения сервисов по нодам с использованием лэйблов, на ноде **master-1** вручную добавлен лэйбл _reliability=high_. Далее в конфигурацию стека в настройки сервиса _mongo_ внесены настройки размещения с ограничениями по созданному лейблу. В результате этого сервис _mongo_ будет размещаться только на нодах с лейблом _reliability=high_, т.е. на ноде master-1.
Аналогичным образом для сервсов ui, post и comment были сконфигуриованы настройки размещения на нодах кластера с ролью **worker**. Сделано это с использованием встроенного лейбла _node.role == worker_.
Кластер перезапущен и задачи на нем размещены в соответствии с заданными ограничениями.
## Масштабирование сервисов.
Для масштабирования приложения сервисы ui, post и comment были дополнены директивами *deploy:* с параметрами типа масштабирования - replicated и колличеством реплик сервиса - 2, для каждого сервиса. После перезапуска каждый сервис запустился в двух екземпярах распределившись по двум нодам каждый. Были опробованы команды управления количеством реплик сервисов на запущеном кластере: *docker stack update* и *docker stack scale*.
Размещения сервиса *node-exporter* сконфигурировано в режиме **global**, в результате данный сервис был размещен на каждой ноде кластера, что необходимо для мониторринга этих нод. 
### Самостоятельное задание.
В swarm-кластер добавлена ещё одна worker-нода **worker-3**. После добавления, на worker-3 сразу развернулся контейнер **node-exporter**, т.к. его масштабирование сконфигурировано с опцией _Global_. После увеличения количества сервисов ui, post и comment до 3 каждый новый сервис разместился на новой ноде worker-3, распространяясь равномерно в соответствии с опцией replicated, и констрейнтом node.role=worker. Измененная схема деплоя отражена в compose-файле **docker-compose.yml**.
### Задание со *.
Вывод можно сделать такой: Global-режим распределения сервисов запускает задачу в единственном экземпляре на каждой ноде, и может использоваться для того чтобы без вмешательства автоматически запускать сервис на каждой ноде, отвечающей условиям. Это способ обеспечить однообразное конфигурирование каждой ноды кластера, и может быть использовано например для запуска сервисов мониторинга на каждой ноде. 
В свою очередь режим деплоя replicated можно использовать для создания сервисов с целью высокой производительности (high-perfomance) в случае ограничения целевых нод производительными системами, или для обеспечения высокой доступности сервисов (high-aviability) в случае использования данного типа распределения без строгих ограничений, что позволит распределить сервисы по всем\группе нодам\нод кластера. Однако запуск сервисов инициируется вручную, либо требуется переконфигурация.
## Балансировка сервисов.
Была прослежена _round-robin_ балансировка сервисов по нодам, которую обеспечивает механизм _routing mesh_. При подключении к сервису *ui* на разных внешних адресах нод кластера с интервалом в несоклько секунд, id контейнеров менялись, что продемонстрировало работу балансировщика который отправляет каждый новый запрос на следующий endpoint.
## Настройка стратегии обновления.
Для обесепечения минимального времени простоя при обновлении сервисов были сконфигурированы параметры директивы *update_config*:
* для сервиса ui - обновление по одному контейнеру, с задежкой в 5 секунд и остановкой деплоя в случае ошибки;
* для сервисов post и comment - обновление по 2 контейнера, с задержкой в 10 секунд и откатом изменений в случае ошибок.
## Органичение ресурсов.
С целью конфигурирования выделения ресурсов для каждого сервиса была использована директива _resources_ с параметрами:
* для сервсиа ui - выделение 25% процессорного времени и 150Mb памяти на экземпяр сервиса;   
* для сервсов post и comment - выделение 30% процессорного времени и 300Mb памяти на экземпяр сервиса.
## Политика перезапуска.
Так как заданное количество экземпляров сервиса поддерживается автоматически, перезапуск контейнеров остановленных по каким-либо причинам будет происходить автоматически. 
Чтобы изменить это поведение была задана конфигурация политики перезапуска контейнера ui с помощью директивы _restart_policy_. Количество попыток перезапуска было ограничено 3-мя, и задан интервал перезапуска в 3 секунды. 
Аналогичным образом была настроена политика перезапуска для сервисов post и comment. Только количество попыток перезапуска было ограничено 12-ю, и задан интервал перезапуска в 1 секунду.
## Самостоятельное задание.
Создан конфигурационный файл **docker-compose.monitoring.yml** В который перенесены все сервисы мониторинга, в том числе конфиг сервиса _node-exporter_ из файла **docker-compose.yml** был перенесен в новый файл. Сервисы prometheus, grafana и alertmanager были размещены на высокопроизводительной ноде *master-1*, а сервисы *node-exporter* и *cadvisor* размещены на всех хостах с применением опции _global_. Для сервиса *prometheus* был собран новый образ *azhelezov/prometheus_swarm* в котором изменена конфигурация таким образом что целевые хосты с которых собираются данные задаются динамически из DNS-сервиса swarm-кластера, а не статически по именам сервисов (ui, post, localhost и тд.). Т.о. данные с хостов собираются отдельно с каждого, а не в общую абстрактную сущность.
Ссылка на новый образ сервиса prometheus: https://hub.docker.com/r/azhelezov/prometheus_swarm/
## Задание ***.
Для разворачивания нескольких окружений я использовал следующую схему. 
Были созданы файлы переменных среды **prod/.env** и **dev/.env** относительно рабочего каталога в котором располагаются файлы **docker-compose...yml**. В них определены переменные необходимые для запуска разных окружений, для продакшена и для разработки соответственно. 
Образцы данных файлов располагаются в дистрибутиве **prod/.env.example** и **dev/.env.example**. Различия между окружениями состоят в разведении по разным портам на которые публикуются сервисы на физических нодах, а так же есть отличия в выделяемых ресурсах. Всё это отражено в файлах переменных окружения. 
Запуск различных окружений я производил из директории проекта следующей командой:
* _docker stack deploy --compose-file=<(cd prod; docker-compose -f ../docker-compose.monitoring.yml -f ../docker-compose.yml config 2>/dev/null; cd ..)  PROD_ - для продакшн окружения;
* _docker stack deploy --compose-file=<(cd dev; docker-compose -f ../docker-compose.monitoring.yml -f ../docker-compose.yml config 2>/dev/null; cd ..)  DEV_ - для окружения разработки.

#######################################################

# logging-1. Домашнее задание #25.

Для ДЗ подготовлено 3 образа контейнеров с новой версией сервисов приложения, для работы с логированием. Для установки пакетов python необходимо было модифидицировать Dockerfile используемый для билда образа сервиса **post**, для этого в Dockerfile строка установки пакета _build-base_ для _apline linux_: **RUN apk update && apk add build-base**. Так же понадобилось внести изменения в код сервиса _post_ в файле **src/post-py/post_app.py**, там был изменен тип данных присваиваемого значения, чтобы решить проблему ошибки типа данных при запуске указанного сервиса. Ещё потребовалось, забежав немного вперед по презентации, добавить сервис _zipkin_ в docker-compose.yml, т.к. сервис _post_ при отсутствии Zipkin выдавал ошибку и при запуске и не работал.
## Визуализация логов.
Уже после запуска всех контейнеров при попытке создать индекс в интерфейсе Kibana, не получалось задать патерн, т.к. интерфейс всё время отвечал что нет такого индекса, соответствующего паттерну _fluentd-*_ пока разбирасля в чем может быть дело проверяя все контейнеры и сервисы, увидел что в логе _elasticsearch_ контейнера произошло событие создания индекса с указанным патерном, после чего в интерфейсе Kibana удалось зарегистрировать соответствующий дефолтный индекс. Видимо проблема была в задержке запуска Elastic. 
В разделе _Discover_ интерфейса Kibana появились логи из централизованой системы Elasicsearch. По доступным полям лога произведен поиск.
### Фильтры.
В описание сервиса **post** добавлено описание драйвера логгинга _fluentd_. В файл **docker/fluentd/fluent.conf** добавлен фильтр который для логов с полем **@log_name:**service.post распарсивает значение поля **log** на отдельные поля. Это позволило производить поиск по новым полям, например: _params.title:*kibana_ - что выводит логи в которых записано событие о создании поста с названием оканчивающимся на _kibana_.
### Неструктурированные логи.
В описание сервиса **ui** добавлено описание драйвера логгинга _fluentd_.
В файл **docker/fluentd/fluent.conf** добавлен фильтр неструктурированных логов который в логах сервиса _ui_ поле _log_ разделяет на отдельные поля при помощи парсинга регулярными выражениями.
Фильтр на основе парсинга регулярными выражениями заменен на grok-фильтр. Результатом парсинга информации из лога фильтром grok было в т.ч. поле _message_. Добавлен второй фильтр-grok который парсит это поле _message_. 
### Задание со звездочкой к неструктурированым логам.
Добавлен grok-фильтр в файл конфигурации **docker/fluentd/fluent.conf** который парсит, оставщийся не распарсеным, лог ui-сервиса.
### Задание с ***
Развернута инфраструктура сервисов приложения с ошибочным кодом. Инфраструктура мониторинга уже была развернута с контейнером zipkin (см. раздел **Визуализация логов**).
Описание проблемы направлено @postgred (Andrey Aleksandrov) в слак чат.
#######################################################

# monitoring-2. Домашнее задание #23.

Для ДЗ подготовлено окружение в виде docker-host: **vm1** на котором развернуты контейнеры микросервисов приложения **reddit**, а так же мониторинг состоящий из контейнеров Prometheus и node-exporter контейнер для наблюдения за метриками **docker-host**.
## Мониторинг контейнеров.
Инфраструктура была преобразована. Описания контейнеров приложеня остались в файле **docker-compose.yml**, а описания контейнеров для реализации мониторинга (prometheus и node-exporter) были вынесены в файл **docker-compose-monitoring.yml**.
В файл **docker-compose-monitoring.yml** добавлен сервис _cadvisor_, job с новым сервисом добавлен в конфигурацию Prometheus, образ контейнера с новой конфигурацией **prometheus.yml** пересобран. После чего инфраструктура контейнеров приложения и мониторинга были пересозданы. Так же в настройки фаервола GCP добавлено правило для доступа к веб-интерфейсу cadvisor'а. Далее попав в UI cadvisor'а убеждаемся в наличии информации о контейнерах запущеных на нашем docker-host и видим информацию по каждому контейнеру. Так же убеждаемся в наличии по пути _IP:8080/metrics_ метрик для Prometheus, и что эти метрики доступны в интерфейсе самого Prometheus.
## Визуализация метрик.
В файл **docker-compose-monitoring.yml** добавлен сервис _grafana_. Инфраструктура с новым контейнером пересобрана. Добавлено правило фаервола GCP для доступа к веб-интерфейсу Grafana, и переходим в UI.
В UI Grafana производим следующие действия:
* Настраиваем источник данных Prometheus.
* Скачиваем из библиотеки комьюнити и схраняем себе файл дашборда **DockerMonitoring.json** для мониторинга Docker. Файл сохраняем в новую директорию **monitoring/grafana/dashboards** репозитория.
* Импортируем скачанный дашборд в Grafana, используя ранее настроенный источник данных Prometheus.
В результате в UI Grafana мы видим информацию о состоянии хоста и контейнеров.
### Метрики приложения.
В конфигурацию Prometheus добавлена _job_name: 'post'_ которая будет собирать метрики с сервиса post нашего приложения. Образ контейнера и инфраструктура мониторинга пересобраны. 
Далее создан дашборд _UI_Service_Monitoring_ в который добавлен график _UI HTTP Requests_ счетчика запросов в разрезе URI-путей на которые они направлены. Для проверки работы данного графика создано несколько постов и коментов в приложении. 
Добавлен график _Rate UI HTTP Requests with Error_ отображения скорости изменения счетчика ошибочных запросов. Этот график наглядно отображает наличие ошибочных запросов в определенный период, в разрезе путей на которые они направлены. График постоен из мерики счетчика ошибочных запросов в качестве аргумента функции rate(). Дашборд сохранен в текущей версии.
Функция rate() использована для преобразования первого графика в более информативный график изменения счетчика количества запросов, аналогично графику _Rate UI HTTP Requests with Error_. Обновленный дашборд сохранен в новой версии.
Изучены тип метрики _histogram_ и понятие _перцентиль_.
Добавлен график _HTTP response time 95th percentile_, отображающий время ответа на запросы у большинства запросов в период времени. Дашборд сохранен и экспортирован в файл **UI_Service_Monitoring.json**.
### Метрики бизнес логики.
Для мониторинга бизнес логики создан дашборд _Business_Logic_Monitoring_ в который добавлен график отображения скорости изменения счетчика постов в приложении. Далее добавлен аналогичный график для коментов. Данны графики наглядно отбражают активность пользователей в промежуток времени. Дашборд сохранен и экспортирован в файл **Business_Logic_Monitoring.json**.
## Алертинг.
Создан образ контейнера alertmanager из файла **monitoring/alertmanager/Dockerfile**. Образ запушен на докер-хаб (ссылки внизу). В качестве мэсенджера использован именной канал в Слаке, для чего на нем настроен web-hook, его API URL использован для настройки alertmanager. Сам alertmanager запущен в ещё одном контейнере, для чего был собран образ с конфигом в котором настроен web-hook и добавлен новый сервис в **docker-compose-monitoring.yml**. В Prometheus добавлена конфигурация алерта, путем добавления файла **monitoring/prometheus/alerts.yml** в образ контейнера. После пересоздания инфраструктуры мониторинга в интерфейсе Alerts появился наш настроенный алерт. Так же его можно посмотреть в собственном UI Alertmanager'а, для чего добавлено правило фаервола GCP для доступа к веб-интерфейсу Alertmanager'а. Для проверки работы алерта был выключен контейнер post, и через минуту получено оповещение об этом в Слак чат. 
## Результаты работы.
Созданные в результате выполнения ДЗ образы находятся по следующим ссылкам:
* https://hub.docker.com/r/azhelezov/comment/
* https://hub.docker.com/r/azhelezov/post/
* https://hub.docker.com/r/azhelezov/ui/
* https://hub.docker.com/r/azhelezov/prometheus/
* https://hub.docker.com/r/azhelezov/alertmanager/

#######################################################

# monitoring-1. Домашнее задание #21.

## Подготовка инфраструктуры.
Для выполнения ДЗ в GCP создан docker-host **vm1** и окружение docker-client переключено на работу с ним. С помощью готового образа с Docker Hub был запущен контейнер с Prometheus.
Подключившись к веб-интерфейсу Prometheus, изучаем интерфейс. И останавливаем контейнер.
### Настройка Prometheus и приложения.
Структура каталогов в репозитории приведена к новому виду, описанному в ДЗ-презентации.
В новом каталоге ./monitoring/Prometheus/ создан Dockerfile для билда образа контейнера с сервером **Prometheus**, настроенным на мониторинг микросервисов нашего приложения. Так же был создан фай конфигурации Prometheus'а **monitoring/prometheus/prometheus.yml**. Далее из указанного контекста был собран образ. 
Так же собраны образы микросервисов с healthcheck'ами.
Файл docker/docker-compose.yml был доработан. В него был добавлен сервис для запуска контейнера с Prometheus, удалены директивы build для сборки имеджей микросервисов, добавлен volume _prometheus_data_, добавлена секция networks в определение сервиса Prometheus. Так же в файле .env изменены ссылки на версии микросервисов на "latest". После запуска контейнеров проверено что приложение работает, и нитерфейс Prometheus отдает 3 endpoint'а.
## Работа с Prometheus.
### Мониторинг сервисов.
Для тестирования встроенных в код микросервисов healthckeck'ов в интерфейсе Prometheus'а были открыты графики соответствующих метрик и проведены поочередные отключения и включения контейнеров с микросервисами. В процессе были отмечены соответствующие переключения метрик в состояние "0", т.е. ошибки доступности как самих сервисов так и зависимых от них.
### Сбор метрик хоста.
Для сбора метрик хоста создан ещё один контейнер с приложением экспортером **node-exporter**. Описание этого контейнера было добавлено в файл docker/docker-compose.yml. В описание данного контейнера добавлены сети сервисов.
В конфиг Prometheus добавлен ещё один job, в котором указываем ссылку на интефейс с метриками. Так же потребовалась пересборка образа с Prometheus, и перезапуск нашей инфраструктуры с новым образом Prometheus. 
После перезапуска инфраструктуры в интерфейсе Prometheus появился новый endpoint **node**. Наблюдая за метрикой _node_load1_ проведено некоторое колличество нагрузочных тестов. В результате были  отмечены соответствующие изменения графика нагрузки на CPU docker-хоста **vm1**.
## Результаты работы.
Созданные в результате выполнения ДЗ образы находятся по следующим ссылкам:
* https://hub.docker.com/r/azhelezov/ui/
* https://hub.docker.com/r/azhelezov/post/
* https://hub.docker.com/r/azhelezov/comment/
* https://hub.docker.com/r/azhelezov/prometheus/


#######################################################

# docker-7. Домашнее задание #20.

## Расширение пайплайна.
Для дальнейшей работы в текущем ДЗ№20 был создан новый проект в группе homework Gitlab CI: **example2**. По причине смены динамического внешнего IP-адреса хоста с Gitlab, потребвалось пересоздавать контейнер _gitlab-ce_ и справленным адресом в _docker-compose.yml_. Так же потребовалось удалить старый _git remote_ и создать новый для проекта из ДЗ№19. Так же новый _git remote_ добавлен для нового проекта **example2**, и к этому же проекту был подключен ранее созданный runner.
### Создание окружений.
Стадия _deploy_job_ в описании пайплайна изменена таким образом что на данной стадии создается откружение _dev_ и на него происходит деплой любых изменений в коде. После пуша изменений на remote **gitlab**, пайплайн отработал без ошибок и появилось окружение dev в списке окружений проекта.
По аналогии с _dev_ окружением, в пайплайн были добавлены определения окружений _staging_ и _production_, которые деплоятся с условием нажатия на кнопку запуска. Тестирование прошло успешно.
### Условия и ограничения.
В окружения _staging_ и _production_ добавлены условия запуска этих job'ов только в случае наличия тэга c версией в semver-формате. Был создан комит без указания тегов и пуш этого комита вызвал пайплайн без job'ов _staging_ и _production_. После чего текущий коммит был изменен, снабжен таким тегом ( 2.4.10 ) и после этого изменения вместе с тегами запушены в remote проекта на gitlab. На этот раз проект собрался со всеми job'ами.
### Динамические окружения.
Для тестирования функционала создания динамических окружений в пайплайн добавлено определение оклужения с динамически назначаемыми именами. Частью составного имени окружения является название ветки полученное из переменной окружения gitlab. При этом условием создания окружения является наличе новой ветки не с названием _master_. Для проверки была создана ветка **new-feature** она была отправлена на удаленную ветку **new-feature** в репозитории gitlab что вызвало запуск пайплайна с созданием окружения _branch/new-feature_.

#######################################################

# docker-6. Домашнее задание #19.

## Подготовка Gitlab CI.
На ресурсах GCP была создана виртуальная машина **docker-gitlab-host** с помощью docker-machine с использованием драйвера google. На ВМ было дополнительно установлено docker-compose, а так же созданы директории необходимые для работы gitlab и установлен gitlab-ce в контейнере (omnibus). После чего интеррфейс gitlab стал доступен по внешнему адресу виртуальной машины из интернет.
После смены пароля root, успешно авторизовавшись, отключили регистрацию новых пользователей.
## Создание проекта.
Через интерфейс Gitlab была создана группа **homework** и проект **example**. После чего проект был подключен как удаленный ресурс к git-репозиторию в ветке docker-6.
Создан файл описания пайплайна _.gitlab-ci.yml_ закомичен в репозиторий и запушен в Gitlab, в результате чего в интерфейсе Gitlab появился наш пайплайн в статусе **pending/stuck**. 
### Запуск и регистрация Runer'а.
В веб интерфейсе Gitlab CI в настройках CI/CD находим и копируем токен ранера, с помощью которого привяжем созданный позже ранер и наш проект.
Далее создаем ранер на **docker-gitlab-host** из докер-образа. При запуске ранера регистрируем его в проекте, используя токен скопированый из интерфейса проекта ранее, а так же http-url по которому доступен наш Gitlab CI. Проверено что новый ранер появился в настройках проекта.
Пробный запуск пайплайна прошел успешно.
## Работа с проектом в Gitlab.
Добавляем тестирование приложения в наш пайплайн.
Исходный код приложения _reddit_ добавлен в локальный репозиторий, и запушен в удаленный gitlab репозиторий **docker-6**. 
Описание пайплайна в _.gitlab-ci.yml_ изменено:
* добавлен docker-image с ruby который будем использовать в тестировании;
* добавлена переменная окружения, указывающая на расположение БД;
* добавлены предварительные скрипты, для установки зависимостей;
* добавлены docker-сервис mongo и вызов скрипта тестирования _simpletest.rb_ в test_unit_job.

В рабочий каталог приложения **reddit** добавлен скрипт _simpletest.rb_.
В **reddit/Gemfile** добалена подключаемая библиотека для ruby: _rack-test_.
После пуша в удаленный репозиторий gitlab docker-6, пайплайн успешно запустился и протестировал наше приложение.

#######################################################

# docker-4. Домашнее задание #17.
## Работа с сетью Docker.
### Сетевой драйвер "none".
Для проверки работы сетевого драйвера "none" использован образ с предустановленными сетевыми утилитами **joffotron/docker-net-tools**. На его основе создан и запущен контейнер. В процессе его работы, подключившись к контейнеру в интерактивном режиме видим что внутри контейнера есть loopback интефейс.
### Сетевой драйвер "host". 
Для проверки работы сетевого драйвера "host" контейнер запущен с параметром _--network host_. После чего были сравнены выводы двух команд _ifconfig_ для запущеных с созданного контейнера "net_test" и затем с хоста "docker-host". Т.к. драйвер "host" использует сетевой стек хоста внутри контейнеров, выводы этих двух команд почти идентичны, вывод _ifconfig_ изнутри контейнера только дополнительно указывает к какому порту прибинден текущий сетевой стек контейнера:
```
...
docker0   Link encap:Ethernet  HWaddr 02:42:DB:90:4C:57       |	docker0   Link encap:Ethernet  HWaddr 02:42:db:90:4c:57  
          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:25	          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:25
          inet6 addr: fe80::42:dbff:fe90:4c57%*32691*/64 Scope: |	          inet6 addr: fe80::42:dbff:fe90:4c57/64 Scope:Link
...
```  
Так же для проверки работы драйвера "host" были несколько раз созданы контейнеры nginx: _docker run --network host -d nginx_. В результате каждый следующий контейнер создаваясь пытался занять всё тот же порт tcp/80 хоста что и предыдущий, в результате процесс nginx контейнера завершался с ошибкой, а с ним и сам контейнер переставал работать.
### Сетевой драйвер "bridge". 
#### Работа внутри одной сети.
Для проверки работы серевого драйвера "bridge" были запущены контейнеры с микросервисами нашего приложения, как и в прошлом ДЗ, но без указания сетевых алиасов, поэтому сервисы не могли общаться друг с другом. После указания алиасов при перезапуске контейнеров приложение заработало.
#### Работа в двух сетях.
Были созданы две сети типа "bridge": _back_net_ для контейнера MongoDB  и _front_net_ для ui. Таким образом у фронтэнда не будет доступа к БД. Промежуточные сервисы приложения (их контейнеры) были добавлены изначально только в сеть _back_net_, но при таком варианте у сервисов post и comment небыло доступа к БД. Поэтому уже после создания и запуска контейнеров они (контейнеры post и comment сервисов) были добавлены и во вторую сеть _front_net_. После чего приложение успешно заработало.
## Docker-compose.
На рабочем компьютере установлена утилита **docker-compose**. В каталоге reddit-microservices репозиория с ДЗ создан файл docker-compose.yml. В него добавлено описание всей инфраструктуры контейнеров приложения в т.ч. volume, сеть и сами контейнеры. Далее с использованием этого файла была создана инфраструктура, предварительно была определена переменная окружения $USERNAME с именем пользователя в docker-hub. Вход на веб-интерфейс приложения показал что оно работает. Однако при попытке перейти в созданный пост получаем ошибку приложения. И это не удивительно в docker-compose.yml, у сервиса post_db как минимум не хватает сетевого алиаса comment_db, поэтому сервис comment не знает как обратиться к базе данных. Исправил это недоразумение тем что добавил алиасы в раздел _networks_ сервиса _post_db_. 
### Самостоятельное задание.
1. Описание проекта в файле docker-compose.yml изменено т.о. что сервисы разделены по двум сетям  _back_net_ и _front_net_ и сервису mongo_db назначены необходимые сетевые алиасы. Тестирование показало что приложение полностью работоспособно.
2. Параметризированы некоторые значения в файле docker-compose.yml:
* USERNAME - имя пользователя,
* H_PORT_UI - порт хоста на который сбинден порт сервиса UI,
* C_PORT_UI - порт сервиса UI внутри контейнера,
* UI_VER - версия образа для сборки контейнера ui,
* POST_VER - версия образа для сборки контейнера post,
* COMMENT_VER - версия образа для сборки контейнера comment.
3. Переменные окружения применяемые в docker-compose.yml описаны в файле .env проекта. Образец такого файла выложен в репозиторий под именем .env.example
4. Переменные применяются из этого файла автоматически при условии запуска docker-compose из директории в которой он лежит.

#######################################################

# docker-3. Домашнее задание #16.
Все рабочие файлы текущего репозитория перенесены в каталог **./docker-monolith/**. В корень репозитория добавлен файл **.dockerignore** с исключениями из контекста в т.ч. указанной директории.
## Разбиение приложения на микросервисы.
В корень репозитория распакован каталог с приложением: **reddit-microservices**.
В каталоги с именами частей приложения: **post**, **comment**, **ui** помещены Dockerfile-ы с описанием образов. На основе этих Docerfile были контейнеры для запуска приложения, каждый сервис в своём контейнере. Так же был скачан контейнер с последней версией MongoDB. P.S. сборка контейнера ui началась с первого шага.
Далее была создана одноранговая сеть _reddit_ средствами docker. В неё будут добавлены наши контейнеры с приложением. 
Все контейнеры были запущены в работу в детач режиме. Каждому контейнеру кроме ui присвоены network-алиасы. Они исполняют роль доменных имен, по которым к контейнерам можно обращаться.
Для проверки я подключился к внешнему адресу docker-host на экспонированый порт 9292 к веб-интерфейсу приложения и создал один пост.
## Оптимизация контейнеров.
В процессе пересборки образов неоднократно возникала ошибка нехватки памяти на docker-host. Поэтому тип машины docker-host на GCP был изменен на g1-small.
Dockerfile сервиса ui был изменен и на его основе был собран образ на >300Mb меньше образа первой версии:
```
azhelezov/ui            2.0                 75b9f41ab998        32 seconds ago      453MB
azhelezov/ui            1.0                 630b8791ede8        2 days ago          775MB
```
Данным образом был создан новый контейнер ui и приложение запущено с этим новым контейнером. После перезапуска контейнера с MongoDB все данные записанные ранее в БД пропали, и поста созданого на предыдущем шаге небыло. Чтобы это исправить был создан иподключен volume для хранения данных. Чтобы смонтировать данный volume в директорию БД в контейнере mongo, все контейнеры были перезапущены, а в команду запуска контейнера mongo была добавлена привязка созданного volume к этому контейнеру опция _-v reddit_db:/data/db_.
Для проверки работы volume на запущеном приложении был создан пост в веб-интерфейсе и далее контейнеры были снова перезапущены. На этот раз после перезапуска информация в БД сохранилась - пост был на месте.

#######################################################

# docker-2. Домашнее задание #15.
## Исходные данные.
ОС Ubuntu Linux (xenial) 16.04.
Установлена docker-machine версии 0.13.0.
В GCP создан новый проект: docker-XXXXXXXX. К нему перенастроена конфигурация gcloud.
## Работа с docker-host.
### Создание docker-host.
На GCP с помощью gcloud создана и запущена ВМ с названием docker-host. Проверка командой _docker-machine ls_ показала что хост создался и что он запущен.
Изменено окружение docker-machine на демона docker-host. Таким образом все команды docker будут выполняться демоном на docker-host.
### вопрос со \*.
Разница в выводе двух комманд:
* docker run --rm -ti tehbilly/htop;
* docker run --rm --pid host -ti tehbilly/htop;

,состоит в том что в первом (дефолтном) варинате мы увидим процесс htop созданного контейнера и только его, а во втором случае мы увидим все процессы хоста с которого запускаем контейнер. Происходит так потому что по дефолту контейнер запускается со своим **PID Namespace** и единственный процесс в нем получает PID=1, а во втором случае из-за опции _--pid=host_ процессу из контейнера выделяется PID из общего namespace хоста. И вцелом из контейнера мы получаем доступ к **PID namespace** хоста и видим все его процессы.
### Создание образа с приложением.
В репозитории создан файл конфигурации MongoDB: mongod.conf.
Создан bash-скрипт запуска приложения: start.sh.
Создан файл переменных в котором задается переменная IP-адреса хоста с БД: db_conf. 
Создан файл Dockerfile, в который будет размещено опсание нашего образа. В Dockerfile добавлены следующие комманды:
* задание исходного образа
```
FROM ubuntu:16.04
```
* обновление кэша репозиториев и установка пакетов для работы приложения
```
RUN apt-get update
RUN apt-get install -y mongodb-server ruby-full ruby-dev build-essential git
RUN gem install bundler
```
* закачка приложения в контейнер
```
RUN git clone https://github.com/Artemmkin/reddit.git
```
* копирование файлов конфигурации в контейнер
```
COPY mongod.conf /etc/mongod.conf
COPY db_config /reddit/db_config
COPY start.sh /start.sh
```
* установка зависимостей и насройка прав доступа к скрипту запуска приложения
```
RUN cd /reddit && bundle install
RUN chmod 0777 /start.sh
```
* выполнение старта приложения при старте контейнера
```
CMD ["/start.sh"]
```
На docker-host cоздан образ контейнера **reddit:latest** из описания в Dockerfile. На основе образа **reddit:latest** создан и запущен контейнер. 
При попытке подключиться к приложению, запущеному в контейнере, по внешнему адресу хоста на порт 9292 соединение не устанавливалось из-за отсутствия соответствующего правила фаервола в VPC-разделе GCP. После добавления правила доступ к приложению по http/9292 появился.
## Работа с Docker Hub.
Создана учетная запись на Docker Hub. С использованием этой учетной записи осуществлен логин из контекста docker к Docker Hub.
Далее созданный ранее образ **reddit:latest** был залит на Docker Hub для дальнейшего использования. Для этого локальному образу образу был присвоен тэг удаленного образа **<login_name>/otus-reddit:1.0** и сделан push к удаленному образу.

#######################################################

# docker-1. Домашнее задание #14.
## Исходные данные
ОС Ubuntu Linux (xenial) 16.04.
Установлена community версия Docker 17.12. Тестовый запуск показал что клиентская и серверная часть работают правильно.
## Операции с контейнерами.
После запуска тестового контейнера "Hello world" испробованы команды Docker для вывода списка контейнеров, образов. Затем на основе образа Ubuntu:16.04 было создано два контейнера и опробованы различные опции команды *docker run*.
Далее были опробованы команды для подключения к уже запущеному контейнеру и комбинации выхода из контейнера без его закрытия.
После чего был создан образ на основе контейнера, модифицированного созданием файла /tmp/file. Вывод команды docker images в котором виден созданный образ сохранен в файл docker-1.log.
### задание со *
Сравнение вывода двух команд docker inspect <image_ID> и docker inspect <container_ID> дописано в файл docker-1.log.

Далее испробованы команды завершения работы контейнеров, отображения занимаемого дискового пространства, удаления контейнеров и образов с опциями.

# AndreyZhelezov_microservices
