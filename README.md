#######################################################

# kubernetes-1. Домашнее задание #28.

## Подготовка.
Создан каталог kubernetes в репозитории. В него размещены файлы-заготовки для описания сущностей **Deployments** для приложений ui, post, comment и mongo.
## Прохождение туториала Kubernetes The Hard Way.
Пройден туториал **Kubernetes The Hard Way**. В результате чего последовательно вручную были созданы основные компоненты кластера Kubrnetes на трех управляющих нодах типа **master** и трех нодах типа **worker**. Ноды развернуты на платформе GCP, к ним настроен балансировщик с постоянным внешним адресом, натсроена утилита kubectl для удаленного управления кластером.
## Самостоятельное задание.
В репозитории ДЗ был создан каталог **kubernetes/kubernetes_the_hard_way** в который были помещены все файлы созданные в процессе прохождения туториала.
Так же на созданном кластере были запущены поды deployment-ов для приложений ui, post, comment и mongo, созданные в первом шаге ДЗ. Они успешно запустились в созданном кластере:

```
$ kubectl get pod -a
NAME                                  READY     STATUS    RESTARTS   AGE
busybox-855686df5d-rll4w              1/1       Running   1          1h
comment-deployment-85f97f7c69-mpd27   1/1       Running   0          22m
mongo-deployment-74cccfb8-v8nxj       1/1       Running   0          22m
nginx-8586cf59-djq8p                  1/1       Running   0          1h
post-deployment-5bfc5d9945-p65qz      1/1       Running   0          22m
ui-deployment-d8f8889-vjs8l           1/1       Running   0          23m
```


#######################################################

# swarm-1. Домашнее задание #27.

## Подготовка кластера Docker Swarm.
На платформе GCP создана ВМ(нода) **master-1**. На ней инициирован кластер docker swarm. Соданы ещё две ноды **worker-1** и **worker-2**. При помощи токена полученого на **master-1** ноды **worker-1** и **worker-2** добалены в кластер с ролью _worker_. 
Для запуска стека был подготовлен и переработан файл конфигурации **docker-compose.yml**. Так же были внесены некоторые корректировки в файл переменных окружения. Запуск стека производился командой *docker stack deploy* и в качестве *--compose-file* указан вывод опции *config* команды *docker-compose*. Такой прием используется потому что *docker stack deploy* не раскрывает переменные из файлов конфигурации docker-compose. В результате был развертнут стек с нашим приложением, состоящим из микросервисов.
## Размещение сервисов.
Для размещения сервисов по нодам с использованием лэйблов, на ноде **master-1** вручную добавлен лэйбл _reliability=high_. Далее в конфигурацию стека в настройки сервиса _mongo_ внесены настройки размещения с ограничениями по созданному лейблу. В результате этого сервис _mongo_ будет размещаться только на нодах с лейблом _reliability=high_, т.е. на ноде master-1.
Аналогичным образом для сервсов ui, post и comment были сконфигуриованы настройки размещения на нодах кластера с ролью **worker**. Сделано это с использованием встроенного лейбла _node.role == worker_.
Кластер перезапущен и задачи на нем размещены в соответствии с заданными ограничениями.
## Масштабирование сервисов.
Для масштабирования приложения сервисы ui, post и comment были дополнены директивами *deploy:* с параметрами типа масштабирования - replicated и колличеством реплик сервиса - 2, для каждого сервиса. После перезапуска каждый сервис запустился в двух екземпярах распределившись по двум нодам каждый. Были опробованы команды управления количеством реплик сервисов на запущеном кластере: *docker stack update* и *docker stack scale*.
Размещения сервиса *node-exporter* сконфигурировано в режиме **global**, в результате данный сервис был размещен на каждой ноде кластера, что необходимо для мониторринга этих нод. 
### Самостоятельное задание.
В swarm-кластер добавлена ещё одна worker-нода **worker-3**. После добавления, на worker-3 сразу развернулся контейнер **node-exporter**, т.к. его масштабирование сконфигурировано с опцией _Global_. После увеличения количества сервисов ui, post и comment до 3 каждый новый сервис разместился на новой ноде worker-3, распространяясь равномерно в соответствии с опцией replicated, и констрейнтом node.role=worker. Измененная схема деплоя отражена в compose-файле **docker-compose.yml**.
### Задание со *.
Вывод можно сделать такой: Global-режим распределения сервисов запускает задачу в единственном экземпляре на каждой ноде, и может использоваться для того чтобы без вмешательства автоматически запускать сервис на каждой ноде, отвечающей условиям. Это способ обеспечить однообразное конфигурирование каждой ноды кластера, и может быть использовано например для запуска сервисов мониторинга на каждой ноде. 
В свою очередь режим деплоя replicated можно использовать для создания сервисов с целью высокой производительности (high-perfomance) в случае ограничения целевых нод производительными системами, или для обеспечения высокой доступности сервисов (high-aviability) в случае использования данного типа распределения без строгих ограничений, что позволит распределить сервисы по всем\группе нодам\нод кластера. Однако запуск сервисов инициируется вручную, либо требуется переконфигурация.
## Балансировка сервисов.
Была прослежена _round-robin_ балансировка сервисов по нодам, которую обеспечивает механизм _routing mesh_. При подключении к сервису *ui* на разных внешних адресах нод кластера с интервалом в несоклько секунд, id контейнеров менялись, что продемонстрировало работу балансировщика который отправляет каждый новый запрос на следующий endpoint.
## Настройка стратегии обновления.
Для обесепечения минимального времени простоя при обновлении сервисов были сконфигурированы параметры директивы *update_config*:
* для сервиса ui - обновление по одному контейнеру, с задежкой в 5 секунд и остановкой деплоя в случае ошибки;
* для сервисов post и comment - обновление по 2 контейнера, с задержкой в 10 секунд и откатом изменений в случае ошибок.
## Органичение ресурсов.
С целью конфигурирования выделения ресурсов для каждого сервиса была использована директива _resources_ с параметрами:
* для сервсиа ui - выделение 25% процессорного времени и 150Mb памяти на экземпяр сервиса;   
* для сервсов post и comment - выделение 30% процессорного времени и 300Mb памяти на экземпяр сервиса.
## Политика перезапуска.
Так как заданное количество экземпляров сервиса поддерживается автоматически, перезапуск контейнеров остановленных по каким-либо причинам будет происходить автоматически. 
Чтобы изменить это поведение была задана конфигурация политики перезапуска контейнера ui с помощью директивы _restart_policy_. Количество попыток перезапуска было ограничено 3-мя, и задан интервал перезапуска в 3 секунды. 
Аналогичным образом была настроена политика перезапуска для сервисов post и comment. Только количество попыток перезапуска было ограничено 12-ю, и задан интервал перезапуска в 1 секунду.
## Самостоятельное задание.
Создан конфигурационный файл **docker-compose.monitoring.yml** В который перенесены все сервисы мониторинга, в том числе конфиг сервиса _node-exporter_ из файла **docker-compose.yml** был перенесен в новый файл. Сервисы prometheus, grafana и alertmanager были размещены на высокопроизводительной ноде *master-1*, а сервисы *node-exporter* и *cadvisor* размещены на всех хостах с применением опции _global_. Для сервиса *prometheus* был собран новый образ *azhelezov/prometheus_swarm* в котором изменена конфигурация таким образом что целевые хосты с которых собираются данные задаются динамически из DNS-сервиса swarm-кластера, а не статически по именам сервисов (ui, post, localhost и тд.). Т.о. данные с хостов собираются отдельно с каждого, а не в общую абстрактную сущность.
Ссылка на новый образ сервиса prometheus: https://hub.docker.com/r/azhelezov/prometheus_swarm/
## Задание ***.
Для разворачивания нескольких окружений я использовал следующую схему. 
Были созданы файлы переменных среды **prod/.env** и **dev/.env** относительно рабочего каталога в котором располагаются файлы **docker-compose...yml**. В них определены переменные необходимые для запуска разных окружений, для продакшена и для разработки соответственно. 
Образцы данных файлов располагаются в дистрибутиве **prod/.env.example** и **dev/.env.example**. Различия между окружениями состоят в разведении по разным портам на которые публикуются сервисы на физических нодах, а так же есть отличия в выделяемых ресурсах. Всё это отражено в файлах переменных окружения. 
Запуск различных окружений я производил из директории проекта следующей командой:
* _docker stack deploy --compose-file=<(cd prod; docker-compose -f ../docker-compose.monitoring.yml -f ../docker-compose.yml config 2>/dev/null; cd ..)  PROD_ - для продакшн окружения;
* _docker stack deploy --compose-file=<(cd dev; docker-compose -f ../docker-compose.monitoring.yml -f ../docker-compose.yml config 2>/dev/null; cd ..)  DEV_ - для окружения разработки.

#######################################################

# logging-1. Домашнее задание #25.

Для ДЗ подготовлено 3 образа контейнеров с новой версией сервисов приложения, для работы с логированием. Для установки пакетов python необходимо было модифидицировать Dockerfile используемый для билда образа сервиса **post**, для этого в Dockerfile строка установки пакета _build-base_ для _apline linux_: **RUN apk update && apk add build-base**. Так же понадобилось внести изменения в код сервиса _post_ в файле **src/post-py/post_app.py**, там был изменен тип данных присваиваемого значения, чтобы решить проблему ошибки типа данных при запуске указанного сервиса. Ещё потребовалось, забежав немного вперед по презентации, добавить сервис _zipkin_ в docker-compose.yml, т.к. сервис _post_ при отсутствии Zipkin выдавал ошибку и при запуске и не работал.
## Визуализация логов.
Уже после запуска всех контейнеров при попытке создать индекс в интерфейсе Kibana, не получалось задать патерн, т.к. интерфейс всё время отвечал что нет такого индекса, соответствующего паттерну _fluentd-*_ пока разбирасля в чем может быть дело проверяя все контейнеры и сервисы, увидел что в логе _elasticsearch_ контейнера произошло событие создания индекса с указанным патерном, после чего в интерфейсе Kibana удалось зарегистрировать соответствующий дефолтный индекс. Видимо проблема была в задержке запуска Elastic. 
В разделе _Discover_ интерфейса Kibana появились логи из централизованой системы Elasicsearch. По доступным полям лога произведен поиск.
### Фильтры.
В описание сервиса **post** добавлено описание драйвера логгинга _fluentd_. В файл **docker/fluentd/fluent.conf** добавлен фильтр который для логов с полем **@log_name:**service.post распарсивает значение поля **log** на отдельные поля. Это позволило производить поиск по новым полям, например: _params.title:*kibana_ - что выводит логи в которых записано событие о создании поста с названием оканчивающимся на _kibana_.
### Неструктурированные логи.
В описание сервиса **ui** добавлено описание драйвера логгинга _fluentd_.
В файл **docker/fluentd/fluent.conf** добавлен фильтр неструктурированных логов который в логах сервиса _ui_ поле _log_ разделяет на отдельные поля при помощи парсинга регулярными выражениями.
Фильтр на основе парсинга регулярными выражениями заменен на grok-фильтр. Результатом парсинга информации из лога фильтром grok было в т.ч. поле _message_. Добавлен второй фильтр-grok который парсит это поле _message_. 
### Задание со звездочкой к неструктурированым логам.
Добавлен grok-фильтр в файл конфигурации **docker/fluentd/fluent.conf** который парсит, оставщийся не распарсеным, лог ui-сервиса.
### Задание с ***
Развернута инфраструктура сервисов приложения с ошибочным кодом. Инфраструктура мониторинга уже была развернута с контейнером zipkin (см. раздел **Визуализация логов**).
Описание проблемы направлено @postgred (Andrey Aleksandrov) в слак чат.
#######################################################

# monitoring-2. Домашнее задание #23.

Для ДЗ подготовлено окружение в виде docker-host: **vm1** на котором развернуты контейнеры микросервисов приложения **reddit**, а так же мониторинг состоящий из контейнеров Prometheus и node-exporter контейнер для наблюдения за метриками **docker-host**.
## Мониторинг контейнеров.
Инфраструктура была преобразована. Описания контейнеров приложеня остались в файле **docker-compose.yml**, а описания контейнеров для реализации мониторинга (prometheus и node-exporter) были вынесены в файл **docker-compose-monitoring.yml**.
В файл **docker-compose-monitoring.yml** добавлен сервис _cadvisor_, job с новым сервисом добавлен в конфигурацию Prometheus, образ контейнера с новой конфигурацией **prometheus.yml** пересобран. После чего инфраструктура контейнеров приложения и мониторинга были пересозданы. Так же в настройки фаервола GCP добавлено правило для доступа к веб-интерфейсу cadvisor'а. Далее попав в UI cadvisor'а убеждаемся в наличии информации о контейнерах запущеных на нашем docker-host и видим информацию по каждому контейнеру. Так же убеждаемся в наличии по пути _IP:8080/metrics_ метрик для Prometheus, и что эти метрики доступны в интерфейсе самого Prometheus.
## Визуализация метрик.
В файл **docker-compose-monitoring.yml** добавлен сервис _grafana_. Инфраструктура с новым контейнером пересобрана. Добавлено правило фаервола GCP для доступа к веб-интерфейсу Grafana, и переходим в UI.
В UI Grafana производим следующие действия:
* Настраиваем источник данных Prometheus.
* Скачиваем из библиотеки комьюнити и схраняем себе файл дашборда **DockerMonitoring.json** для мониторинга Docker. Файл сохраняем в новую директорию **monitoring/grafana/dashboards** репозитория.
* Импортируем скачанный дашборд в Grafana, используя ранее настроенный источник данных Prometheus.
В результате в UI Grafana мы видим информацию о состоянии хоста и контейнеров.
### Метрики приложения.
В конфигурацию Prometheus добавлена _job_name: 'post'_ которая будет собирать метрики с сервиса post нашего приложения. Образ контейнера и инфраструктура мониторинга пересобраны. 
Далее создан дашборд _UI_Service_Monitoring_ в который добавлен график _UI HTTP Requests_ счетчика запросов в разрезе URI-путей на которые они направлены. Для проверки работы данного графика создано несколько постов и коментов в приложении. 
Добавлен график _Rate UI HTTP Requests with Error_ отображения скорости изменения счетчика ошибочных запросов. Этот график наглядно отображает наличие ошибочных запросов в определенный период, в разрезе путей на которые они направлены. График постоен из мерики счетчика ошибочных запросов в качестве аргумента функции rate(). Дашборд сохранен в текущей версии.
Функция rate() использована для преобразования первого графика в более информативный график изменения счетчика количества запросов, аналогично графику _Rate UI HTTP Requests with Error_. Обновленный дашборд сохранен в новой версии.
Изучены тип метрики _histogram_ и понятие _перцентиль_.
Добавлен график _HTTP response time 95th percentile_, отображающий время ответа на запросы у большинства запросов в период времени. Дашборд сохранен и экспортирован в файл **UI_Service_Monitoring.json**.
### Метрики бизнес логики.
Для мониторинга бизнес логики создан дашборд _Business_Logic_Monitoring_ в который добавлен график отображения скорости изменения счетчика постов в приложении. Далее добавлен аналогичный график для коментов. Данны графики наглядно отбражают активность пользователей в промежуток времени. Дашборд сохранен и экспортирован в файл **Business_Logic_Monitoring.json**.
## Алертинг.
Создан образ контейнера alertmanager из файла **monitoring/alertmanager/Dockerfile**. Образ запушен на докер-хаб (ссылки внизу). В качестве мэсенджера использован именной канал в Слаке, для чего на нем настроен web-hook, его API URL использован для настройки alertmanager. Сам alertmanager запущен в ещё одном контейнере, для чего был собран образ с конфигом в котором настроен web-hook и добавлен новый сервис в **docker-compose-monitoring.yml**. В Prometheus добавлена конфигурация алерта, путем добавления файла **monitoring/prometheus/alerts.yml** в образ контейнера. После пересоздания инфраструктуры мониторинга в интерфейсе Alerts появился наш настроенный алерт. Так же его можно посмотреть в собственном UI Alertmanager'а, для чего добавлено правило фаервола GCP для доступа к веб-интерфейсу Alertmanager'а. Для проверки работы алерта был выключен контейнер post, и через минуту получено оповещение об этом в Слак чат. 
## Результаты работы.
Созданные в результате выполнения ДЗ образы находятся по следующим ссылкам:
* https://hub.docker.com/r/azhelezov/comment/
* https://hub.docker.com/r/azhelezov/post/
* https://hub.docker.com/r/azhelezov/ui/
* https://hub.docker.com/r/azhelezov/prometheus/
* https://hub.docker.com/r/azhelezov/alertmanager/

#######################################################

# monitoring-1. Домашнее задание #21.

## Подготовка инфраструктуры.
Для выполнения ДЗ в GCP создан docker-host **vm1** и окружение docker-client переключено на работу с ним. С помощью готового образа с Docker Hub был запущен контейнер с Prometheus.
Подключившись к веб-интерфейсу Prometheus, изучаем интерфейс. И останавливаем контейнер.
### Настройка Prometheus и приложения.
Структура каталогов в репозитории приведена к новому виду, описанному в ДЗ-презентации.
В новом каталоге ./monitoring/Prometheus/ создан Dockerfile для билда образа контейнера с сервером **Prometheus**, настроенным на мониторинг микросервисов нашего приложения. Так же был создан фай конфигурации Prometheus'а **monitoring/prometheus/prometheus.yml**. Далее из указанного контекста был собран образ. 
Так же собраны образы микросервисов с healthcheck'ами.
Файл docker/docker-compose.yml был доработан. В него был добавлен сервис для запуска контейнера с Prometheus, удалены директивы build для сборки имеджей микросервисов, добавлен volume _prometheus_data_, добавлена секция networks в определение сервиса Prometheus. Так же в файле .env изменены ссылки на версии микросервисов на "latest". После запуска контейнеров проверено что приложение работает, и нитерфейс Prometheus отдает 3 endpoint'а.
## Работа с Prometheus.
### Мониторинг сервисов.
Для тестирования встроенных в код микросервисов healthckeck'ов в интерфейсе Prometheus'а были открыты графики соответствующих метрик и проведены поочередные отключения и включения контейнеров с микросервисами. В процессе были отмечены соответствующие переключения метрик в состояние "0", т.е. ошибки доступности как самих сервисов так и зависимых от них.
### Сбор метрик хоста.
Для сбора метрик хоста создан ещё один контейнер с приложением экспортером **node-exporter**. Описание этого контейнера было добавлено в файл docker/docker-compose.yml. В описание данного контейнера добавлены сети сервисов.
В конфиг Prometheus добавлен ещё один job, в котором указываем ссылку на интефейс с метриками. Так же потребовалась пересборка образа с Prometheus, и перезапуск нашей инфраструктуры с новым образом Prometheus. 
После перезапуска инфраструктуры в интерфейсе Prometheus появился новый endpoint **node**. Наблюдая за метрикой _node_load1_ проведено некоторое колличество нагрузочных тестов. В результате были  отмечены соответствующие изменения графика нагрузки на CPU docker-хоста **vm1**.
## Результаты работы.
Созданные в результате выполнения ДЗ образы находятся по следующим ссылкам:
* https://hub.docker.com/r/azhelezov/ui/
* https://hub.docker.com/r/azhelezov/post/
* https://hub.docker.com/r/azhelezov/comment/
* https://hub.docker.com/r/azhelezov/prometheus/


#######################################################

# docker-7. Домашнее задание #20.

## Расширение пайплайна.
Для дальнейшей работы в текущем ДЗ№20 был создан новый проект в группе homework Gitlab CI: **example2**. По причине смены динамического внешнего IP-адреса хоста с Gitlab, потребвалось пересоздавать контейнер _gitlab-ce_ и справленным адресом в _docker-compose.yml_. Так же потребовалось удалить старый _git remote_ и создать новый для проекта из ДЗ№19. Так же новый _git remote_ добавлен для нового проекта **example2**, и к этому же проекту был подключен ранее созданный runner.
### Создание окружений.
Стадия _deploy_job_ в описании пайплайна изменена таким образом что на данной стадии создается откружение _dev_ и на него происходит деплой любых изменений в коде. После пуша изменений на remote **gitlab**, пайплайн отработал без ошибок и появилось окружение dev в списке окружений проекта.
По аналогии с _dev_ окружением, в пайплайн были добавлены определения окружений _staging_ и _production_, которые деплоятся с условием нажатия на кнопку запуска. Тестирование прошло успешно.
### Условия и ограничения.
В окружения _staging_ и _production_ добавлены условия запуска этих job'ов только в случае наличия тэга c версией в semver-формате. Был создан комит без указания тегов и пуш этого комита вызвал пайплайн без job'ов _staging_ и _production_. После чего текущий коммит был изменен, снабжен таким тегом ( 2.4.10 ) и после этого изменения вместе с тегами запушены в remote проекта на gitlab. На этот раз проект собрался со всеми job'ами.
### Динамические окружения.
Для тестирования функционала создания динамических окружений в пайплайн добавлено определение оклужения с динамически назначаемыми именами. Частью составного имени окружения является название ветки полученное из переменной окружения gitlab. При этом условием создания окружения является наличе новой ветки не с названием _master_. Для проверки была создана ветка **new-feature** она была отправлена на удаленную ветку **new-feature** в репозитории gitlab что вызвало запуск пайплайна с созданием окружения _branch/new-feature_.

#######################################################

# docker-6. Домашнее задание #19.

## Подготовка Gitlab CI.
На ресурсах GCP была создана виртуальная машина **docker-gitlab-host** с помощью docker-machine с использованием драйвера google. На ВМ было дополнительно установлено docker-compose, а так же созданы директории необходимые для работы gitlab и установлен gitlab-ce в контейнере (omnibus). После чего интеррфейс gitlab стал доступен по внешнему адресу виртуальной машины из интернет.
После смены пароля root, успешно авторизовавшись, отключили регистрацию новых пользователей.
## Создание проекта.
Через интерфейс Gitlab была создана группа **homework** и проект **example**. После чего проект был подключен как удаленный ресурс к git-репозиторию в ветке docker-6.
Создан файл описания пайплайна _.gitlab-ci.yml_ закомичен в репозиторий и запушен в Gitlab, в результате чего в интерфейсе Gitlab появился наш пайплайн в статусе **pending/stuck**. 
### Запуск и регистрация Runer'а.
В веб интерфейсе Gitlab CI в настройках CI/CD находим и копируем токен ранера, с помощью которого привяжем созданный позже ранер и наш проект.
Далее создаем ранер на **docker-gitlab-host** из докер-образа. При запуске ранера регистрируем его в проекте, используя токен скопированый из интерфейса проекта ранее, а так же http-url по которому доступен наш Gitlab CI. Проверено что новый ранер появился в настройках проекта.
Пробный запуск пайплайна прошел успешно.
## Работа с проектом в Gitlab.
Добавляем тестирование приложения в наш пайплайн.
Исходный код приложения _reddit_ добавлен в локальный репозиторий, и запушен в удаленный gitlab репозиторий **docker-6**. 
Описание пайплайна в _.gitlab-ci.yml_ изменено:
* добавлен docker-image с ruby который будем использовать в тестировании;
* добавлена переменная окружения, указывающая на расположение БД;
* добавлены предварительные скрипты, для установки зависимостей;
* добавлены docker-сервис mongo и вызов скрипта тестирования _simpletest.rb_ в test_unit_job.

В рабочий каталог приложения **reddit** добавлен скрипт _simpletest.rb_.
В **reddit/Gemfile** добалена подключаемая библиотека для ruby: _rack-test_.
После пуша в удаленный репозиторий gitlab docker-6, пайплайн успешно запустился и протестировал наше приложение.

#######################################################

# docker-4. Домашнее задание #17.
## Работа с сетью Docker.
### Сетевой драйвер "none".
Для проверки работы сетевого драйвера "none" использован образ с предустановленными сетевыми утилитами **joffotron/docker-net-tools**. На его основе создан и запущен контейнер. В процессе его работы, подключившись к контейнеру в интерактивном режиме видим что внутри контейнера есть loopback интефейс.
### Сетевой драйвер "host". 
Для проверки работы сетевого драйвера "host" контейнер запущен с параметром _--network host_. После чего были сравнены выводы двух команд _ifconfig_ для запущеных с созданного контейнера "net_test" и затем с хоста "docker-host". Т.к. драйвер "host" использует сетевой стек хоста внутри контейнеров, выводы этих двух команд почти идентичны, вывод _ifconfig_ изнутри контейнера только дополнительно указывает к какому порту прибинден текущий сетевой стек контейнера:
```
...
docker0   Link encap:Ethernet  HWaddr 02:42:DB:90:4C:57       |	docker0   Link encap:Ethernet  HWaddr 02:42:db:90:4c:57  
          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:25	          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:25
          inet6 addr: fe80::42:dbff:fe90:4c57%*32691*/64 Scope: |	          inet6 addr: fe80::42:dbff:fe90:4c57/64 Scope:Link
...
```  
Так же для проверки работы драйвера "host" были несколько раз созданы контейнеры nginx: _docker run --network host -d nginx_. В результате каждый следующий контейнер создаваясь пытался занять всё тот же порт tcp/80 хоста что и предыдущий, в результате процесс nginx контейнера завершался с ошибкой, а с ним и сам контейнер переставал работать.
### Сетевой драйвер "bridge". 
#### Работа внутри одной сети.
Для проверки работы серевого драйвера "bridge" были запущены контейнеры с микросервисами нашего приложения, как и в прошлом ДЗ, но без указания сетевых алиасов, поэтому сервисы не могли общаться друг с другом. После указания алиасов при перезапуске контейнеров приложение заработало.
#### Работа в двух сетях.
Были созданы две сети типа "bridge": _back_net_ для контейнера MongoDB  и _front_net_ для ui. Таким образом у фронтэнда не будет доступа к БД. Промежуточные сервисы приложения (их контейнеры) были добавлены изначально только в сеть _back_net_, но при таком варианте у сервисов post и comment небыло доступа к БД. Поэтому уже после создания и запуска контейнеров они (контейнеры post и comment сервисов) были добавлены и во вторую сеть _front_net_. После чего приложение успешно заработало.
## Docker-compose.
На рабочем компьютере установлена утилита **docker-compose**. В каталоге reddit-microservices репозиория с ДЗ создан файл docker-compose.yml. В него добавлено описание всей инфраструктуры контейнеров приложения в т.ч. volume, сеть и сами контейнеры. Далее с использованием этого файла была создана инфраструктура, предварительно была определена переменная окружения $USERNAME с именем пользователя в docker-hub. Вход на веб-интерфейс приложения показал что оно работает. Однако при попытке перейти в созданный пост получаем ошибку приложения. И это не удивительно в docker-compose.yml, у сервиса post_db как минимум не хватает сетевого алиаса comment_db, поэтому сервис comment не знает как обратиться к базе данных. Исправил это недоразумение тем что добавил алиасы в раздел _networks_ сервиса _post_db_. 
### Самостоятельное задание.
1. Описание проекта в файле docker-compose.yml изменено т.о. что сервисы разделены по двум сетям  _back_net_ и _front_net_ и сервису mongo_db назначены необходимые сетевые алиасы. Тестирование показало что приложение полностью работоспособно.
2. Параметризированы некоторые значения в файле docker-compose.yml:
* USERNAME - имя пользователя,
* H_PORT_UI - порт хоста на который сбинден порт сервиса UI,
* C_PORT_UI - порт сервиса UI внутри контейнера,
* UI_VER - версия образа для сборки контейнера ui,
* POST_VER - версия образа для сборки контейнера post,
* COMMENT_VER - версия образа для сборки контейнера comment.
3. Переменные окружения применяемые в docker-compose.yml описаны в файле .env проекта. Образец такого файла выложен в репозиторий под именем .env.example
4. Переменные применяются из этого файла автоматически при условии запуска docker-compose из директории в которой он лежит.

#######################################################

# docker-3. Домашнее задание #16.
Все рабочие файлы текущего репозитория перенесены в каталог **./docker-monolith/**. В корень репозитория добавлен файл **.dockerignore** с исключениями из контекста в т.ч. указанной директории.
## Разбиение приложения на микросервисы.
В корень репозитория распакован каталог с приложением: **reddit-microservices**.
В каталоги с именами частей приложения: **post**, **comment**, **ui** помещены Dockerfile-ы с описанием образов. На основе этих Docerfile были контейнеры для запуска приложения, каждый сервис в своём контейнере. Так же был скачан контейнер с последней версией MongoDB. P.S. сборка контейнера ui началась с первого шага.
Далее была создана одноранговая сеть _reddit_ средствами docker. В неё будут добавлены наши контейнеры с приложением. 
Все контейнеры были запущены в работу в детач режиме. Каждому контейнеру кроме ui присвоены network-алиасы. Они исполняют роль доменных имен, по которым к контейнерам можно обращаться.
Для проверки я подключился к внешнему адресу docker-host на экспонированый порт 9292 к веб-интерфейсу приложения и создал один пост.
## Оптимизация контейнеров.
В процессе пересборки образов неоднократно возникала ошибка нехватки памяти на docker-host. Поэтому тип машины docker-host на GCP был изменен на g1-small.
Dockerfile сервиса ui был изменен и на его основе был собран образ на >300Mb меньше образа первой версии:
```
azhelezov/ui            2.0                 75b9f41ab998        32 seconds ago      453MB
azhelezov/ui            1.0                 630b8791ede8        2 days ago          775MB
```
Данным образом был создан новый контейнер ui и приложение запущено с этим новым контейнером. После перезапуска контейнера с MongoDB все данные записанные ранее в БД пропали, и поста созданого на предыдущем шаге небыло. Чтобы это исправить был создан иподключен volume для хранения данных. Чтобы смонтировать данный volume в директорию БД в контейнере mongo, все контейнеры были перезапущены, а в команду запуска контейнера mongo была добавлена привязка созданного volume к этому контейнеру опция _-v reddit_db:/data/db_.
Для проверки работы volume на запущеном приложении был создан пост в веб-интерфейсе и далее контейнеры были снова перезапущены. На этот раз после перезапуска информация в БД сохранилась - пост был на месте.

#######################################################

# docker-2. Домашнее задание #15.
## Исходные данные.
ОС Ubuntu Linux (xenial) 16.04.
Установлена docker-machine версии 0.13.0.
В GCP создан новый проект: docker-XXXXXXXX. К нему перенастроена конфигурация gcloud.
## Работа с docker-host.
### Создание docker-host.
На GCP с помощью gcloud создана и запущена ВМ с названием docker-host. Проверка командой _docker-machine ls_ показала что хост создался и что он запущен.
Изменено окружение docker-machine на демона docker-host. Таким образом все команды docker будут выполняться демоном на docker-host.
### вопрос со \*.
Разница в выводе двух комманд:
* docker run --rm -ti tehbilly/htop;
* docker run --rm --pid host -ti tehbilly/htop;

,состоит в том что в первом (дефолтном) варинате мы увидим процесс htop созданного контейнера и только его, а во втором случае мы увидим все процессы хоста с которого запускаем контейнер. Происходит так потому что по дефолту контейнер запускается со своим **PID Namespace** и единственный процесс в нем получает PID=1, а во втором случае из-за опции _--pid=host_ процессу из контейнера выделяется PID из общего namespace хоста. И вцелом из контейнера мы получаем доступ к **PID namespace** хоста и видим все его процессы.
### Создание образа с приложением.
В репозитории создан файл конфигурации MongoDB: mongod.conf.
Создан bash-скрипт запуска приложения: start.sh.
Создан файл переменных в котором задается переменная IP-адреса хоста с БД: db_conf. 
Создан файл Dockerfile, в который будет размещено опсание нашего образа. В Dockerfile добавлены следующие комманды:
* задание исходного образа
```
FROM ubuntu:16.04
```
* обновление кэша репозиториев и установка пакетов для работы приложения
```
RUN apt-get update
RUN apt-get install -y mongodb-server ruby-full ruby-dev build-essential git
RUN gem install bundler
```
* закачка приложения в контейнер
```
RUN git clone https://github.com/Artemmkin/reddit.git
```
* копирование файлов конфигурации в контейнер
```
COPY mongod.conf /etc/mongod.conf
COPY db_config /reddit/db_config
COPY start.sh /start.sh
```
* установка зависимостей и насройка прав доступа к скрипту запуска приложения
```
RUN cd /reddit && bundle install
RUN chmod 0777 /start.sh
```
* выполнение старта приложения при старте контейнера
```
CMD ["/start.sh"]
```
На docker-host cоздан образ контейнера **reddit:latest** из описания в Dockerfile. На основе образа **reddit:latest** создан и запущен контейнер. 
При попытке подключиться к приложению, запущеному в контейнере, по внешнему адресу хоста на порт 9292 соединение не устанавливалось из-за отсутствия соответствующего правила фаервола в VPC-разделе GCP. После добавления правила доступ к приложению по http/9292 появился.
## Работа с Docker Hub.
Создана учетная запись на Docker Hub. С использованием этой учетной записи осуществлен логин из контекста docker к Docker Hub.
Далее созданный ранее образ **reddit:latest** был залит на Docker Hub для дальнейшего использования. Для этого локальному образу образу был присвоен тэг удаленного образа **<login_name>/otus-reddit:1.0** и сделан push к удаленному образу.

#######################################################

# docker-1. Домашнее задание #14.
## Исходные данные
ОС Ubuntu Linux (xenial) 16.04.
Установлена community версия Docker 17.12. Тестовый запуск показал что клиентская и серверная часть работают правильно.
## Операции с контейнерами.
После запуска тестового контейнера "Hello world" испробованы команды Docker для вывода списка контейнеров, образов. Затем на основе образа Ubuntu:16.04 было создано два контейнера и опробованы различные опции команды *docker run*.
Далее были опробованы команды для подключения к уже запущеному контейнеру и комбинации выхода из контейнера без его закрытия.
После чего был создан образ на основе контейнера, модифицированного созданием файла /tmp/file. Вывод команды docker images в котором виден созданный образ сохранен в файл docker-1.log.
### задание со *
Сравнение вывода двух команд docker inspect <image_ID> и docker inspect <container_ID> дописано в файл docker-1.log.

Далее испробованы команды завершения работы контейнеров, отображения занимаемого дискового пространства, удаления контейнеров и образов с опциями.

# AndreyZhelezov_microservices
